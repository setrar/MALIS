{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfd9ce9d-d0ca-4aef-a4cd-619b65479036",
   "metadata": {},
   "source": [
    "# Part 1: Implementing Logistic Regression from Scratch\n",
    "\n",
    "In this part of the lab, you will be requested to implement logistic regression from scratch. This means you will need to make use of gradient descent to find the parameters of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cecb4623-29dd-4575-8416-e68b69566488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eb94b38-e6f0-41f5-8f9e-6120eb4afe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(title, values):\n",
    "    '''\n",
    "    This function will allow us to check the evolution of the loss function during gradient descent\n",
    "    Inputs:\n",
    "    Title - title of the plot\n",
    "    Values - values to be plotted\n",
    "    '''\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.plot(values)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c817b9f-6f51-438a-bb80-606770f6de4e",
   "metadata": {},
   "source": [
    "For this exercise, we will make use of the function [make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) from scikit-learn, which generates random data for an k-class classification problem. We will use k=2 to stick to a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99837ce0-d285-472c-9da9-aed7716ad5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_features=10, n_redundant=0, n_informative=6, n_classes=2, n_clusters_per_class=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a438a60-7b45-43cf-998c-4de87091300b",
   "metadata": {},
   "source": [
    "## Question 1: make_classification\n",
    "Check the documentation of the function to determine what is the role of the following parameters:\n",
    "\n",
    "    1. n_redundant\n",
    "    2. n_informative\n",
    "    3. n_repeated\n",
    "\n",
    "Based on your findings, how many useless features does the dataset contain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7ca2f7-d034-48d2-8a88-5b7b443cb205",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Your answer here:\n",
    "\n",
    "\n",
    "_n_features=n_informative+n_redundant+n_repeated_\n",
    "\n",
    "    1. n_redundant:\n",
    "\n",
    "The number of redundant feature, features generated as linear combinations of the informative features\n",
    "\n",
    "    2. n_informative\n",
    "\n",
    "The number or \"informative\" features that are meaningful and contain patterns or signals that are relevant in the current context.\n",
    "\n",
    "    3. n_repeated\n",
    "\n",
    "The number of duplicated features, drawn randomly from the informative and the redundant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdb75a6-1f37-43de-beab-f2e65b93d91e",
   "metadata": {},
   "source": [
    "## Exercise 1: Scaling input features\n",
    "When different input features have extremely different ranges of values, it is common to rescale them so they have comparable ranges. We standardize input values by centering them to result in a zero mean and a standard deviation of one (this transformation is sometimes called the z-score). That is, if $\\mu_j$ is the mean of the values of the j-th feature across the N samples in the input dataset, and $\\sigma_j$ is the standard deviation of the values of features j-th across the input dataset, we can replace each feature $x_i^j$ by a new feature $x^{'j}_i$ computed as follows:\n",
    "\n",
    "$$\\mu_j = \\frac{1}{N}\\sum^{N}_{i=1} x_i^j$$\n",
    "\n",
    "$$\\sigma_j = \\sqrt{\\frac{1}{N}\\sum^{N}_{i=1} x_i^j - \\mu_j)^2} $$\n",
    "\n",
    "$$x^{'j}_i = \\dfrac{x_i^j - \\mu_j}{\\sigma_j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365f0371-223d-4c26-867e-d9837c504f65",
   "metadata": {},
   "source": [
    "### Task 1.1: Implement feature scaling\n",
    "Implement below the function standardize, which estimates the mean and standard deviation of each feature in the dataset and then standardizes all the input features.\n",
    "\n",
    "**Hint:** Check the documentation of the functions mean, std and divide from numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a53e28-c3ed-4aba-a293-02054f79a86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X, mean = None, stdev = None):\n",
    "    '''\n",
    "    Transforms the input data using the z-score. \n",
    "    If the mean and stdev are provided, the function only performs the transformation.\n",
    "    Otherwise, it first estimates the mean and standard deviation\n",
    "    Inputs:\n",
    "    X- Data to standardize\n",
    "    mean - vector with means of each feature (default None)\n",
    "    stdev - vector with standard deviation of each feature (default None)\n",
    "    Outputs:\n",
    "    X_stand - Standardized data\n",
    "    mean - Mean of the data\n",
    "    stdev - standard deviation of the data\n",
    "    '''\n",
    "    #YOUR CODE HERE\n",
    "    \n",
    "    return X_stand, mean, stdev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbcfcca-b506-412f-8c72-04219eb17808",
   "metadata": {},
   "source": [
    "## Exercise 2: Implementing and Running Logistic Regression\n",
    "### Task 2.1: Implement Logistic Regression\n",
    "Below you will see the skeleton of the Logistic Regression class. Some of its functions have already been implemented. Have a look at them and try to understand them.\n",
    "\n",
    "Afterwards, you will need to complete the following:\n",
    "\n",
    "1. **function sigmoid** - Computes the sigmoid function given an input (*see slide 15 of the Logistic Regression slide deck*)\n",
    "\n",
    "2. **function loss_function** - Estimates the cross-entropy loss given an input matrix X, a vector of labels y and the weights. Attention: In the course's slides, we estimated the loss by suming over all elements of the training set. For efficiency purposes, estimate it using matrix/vector computations. You may have a look into the linear regression lab for inspiration on how to do this\n",
    "\n",
    "3. **function gradient_descent_step** - Performs an update of the weights for logistic regression. Using matrix notation this is expressed as:\n",
    "$$ \\mathbf{w}^{(\\tau+1)} = \\mathbf{w}^{(\\tau)} + \\dfrac{\\alpha}{N}\\mathbf{X}^T\\left(\\mathbf{y}-\\sigma\\left(\\mathbf{X}\\mathbf{w}\\right)\\right)$$\n",
    "\n",
    "4. **function prediction** - Predicts new labels y_pred given an input matrix (*see slide 24 from Logistic regression slide deck*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9e3a5a-a678-43ee-beb5-47ed9d6bbcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    def initialize_weights(self,X):\n",
    "        '''\n",
    "        Initializes the parameters so that the have the same dimensions as the input data + 1\n",
    "        Inputs:\n",
    "        X - input data matrix of dimensions N x D\n",
    "        \n",
    "        Outputs:\n",
    "        weights - model parameters initialized to zero size (D + 1) x 1\n",
    "        '''\n",
    "        weights = np.zeros((X.shape[1]+1,1))\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def initialize_X(self,X):\n",
    "        '''\n",
    "        Reshapes the input data so that it can handle w_0\n",
    "        Inputs:\n",
    "        X - input data matrix of dimensions N x D\n",
    "        Outputs:\n",
    "        X - matrix of size N x (D + 1)\n",
    "        '''\n",
    "        X = PolynomialFeatures(1).fit_transform(X) #Adds a one to the matrix so it copes with w_0\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def sigmoid(self,z):\n",
    "        '''\n",
    "        Implements the sigmoid function\n",
    "        Input:\n",
    "        z - input variable \n",
    "        \n",
    "        Output:\n",
    "        1/(1+exp(-z))\n",
    "        '''\n",
    "        # YOUR CODE HERE \n",
    "        sig = 1/(1+np.exp(-z))\n",
    "        return sig\n",
    "        \n",
    "    def loss_function(self,X,y,w):\n",
    "        '''\n",
    "        Implements the cross-entropy loss. See Eq 1, Slide 21 from the Logistic Regression slide deck as a reminder.\n",
    "        Note that the expression in slide 21 is not using a matrix notation. \n",
    "        Input:\n",
    "        X - Input matrix of size N x (D + 1)\n",
    "        y - Label vector of size N x 1\n",
    "        w - Parameters vector of size (D + 1) x 1\n",
    "        \n",
    "        Output: \n",
    "        Estimation of the cross-entropy loss given the input, labels and parameters (scalar value)\n",
    "        '''\n",
    "        \n",
    "        #1) Estimate Xw\n",
    "        #YOUR CODE HERE\n",
    "        \n",
    "        #2) Estimate sigmoid of Xw\n",
    "         #YOUR CODE HERE\n",
    "        \n",
    "        #3) estimate log(sig) and log(1-sig)\n",
    "         #YOUR CODE HERE\n",
    "        \n",
    "        #4) Combine point 3 with the labels and sum over all elements to obtain the final estimate\n",
    "        loss =  #YOUR CODE HERE\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def gradient_descent_step(self,X, y, w, alpha):\n",
    "        '''\n",
    "        Implements a gradient descent step for logistic regression\n",
    "        Input:\n",
    "        X - Input matrix of size N x (D + 1)\n",
    "        y - Label vector of size N x 1\n",
    "        w - Parameters vector of size (D + 1) x 1\n",
    "        alpha - Learning rate \n",
    "        Output: \n",
    "        Updated weights\n",
    "        '''\n",
    "        \n",
    "        w = #YOUR CODE HERE\n",
    "        \n",
    "        return w\n",
    "    \n",
    "    def fit(self,X,y,alpha=0.01,iter=10, epsilon = 0.0001):\n",
    "        '''\n",
    "        Learning procedure of the logistic regression model\n",
    "        Input:\n",
    "        X - Input matrix of size N x (D + 1)\n",
    "        y - Label vector of size N x 1\n",
    "        alpha - Learning rate (default value 0.01)\n",
    "        iter - Number of iterations to perform for gradient descent (default 10)\n",
    "        epsilon - stopping criterion (default 0.0001)\n",
    "        Output: \n",
    "        List of values of the loss function during the gradient descent iterations\n",
    "        '''\n",
    "        weights = self.initialize_weights(X) #Initializes the weights of the model\n",
    "        X = self.initialize_X(X) #reformats X\n",
    "        \n",
    "        \n",
    "        loss_list = np.zeros(iter,) # We will store the values of the loss function as gradient descent advances\n",
    "        \n",
    "        for i in range(iter):\n",
    "            weights = self.gradient_descent_step(X, y, weights, alpha)\n",
    "            \n",
    "            loss_list[i] = self.loss_function(X,y,weights)\n",
    "            \n",
    "            if loss_list[i] <= epsilon:\n",
    "                break\n",
    "            \n",
    "        self.weights = weights\n",
    "        \n",
    "        return loss_list\n",
    "    \n",
    "    def predict(self,X):\n",
    "        '''\n",
    "        Predicts labels y given an input matrix X\n",
    "        Input: \n",
    "        X- matrix of dimensions N x D\n",
    "        \n",
    "        Output:\n",
    "        y_pred - vector of labels (dimensions N x 1)\n",
    "        '''\n",
    "        #1) Reformat the matrix X\n",
    "        #YOUR CODE HEREX)\n",
    "        \n",
    "        #2) Estimate Xweights\n",
    "        #YOUR CODE HERE\n",
    "        \n",
    "        #3) Use slide 24 from the slide deck to assign the labels y\n",
    "        #YOUR CODE HERE\n",
    "        \n",
    "        return y_pred.astype(int)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f969c8e4-be55-4570-a76e-59df331f2c49",
   "metadata": {},
   "source": [
    "We are now ready to test your implementation of Logistic Regression. Go through the different steps below and understand what exactly is being done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f645736c-8790-4947-9446-ba09f871b8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we split the data into two sets: training and testing (no validation set in this lab)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X,y,test_size=0.1)\n",
    "\n",
    "# Next we standardize the training set\n",
    "X_tr, mean, std = standardize(X_tr)\n",
    "\n",
    "# The test input features are standardized using the mean and std computed on the training set \n",
    "X_te, _, _ = standardize(X_te, mean, std)\n",
    "\n",
    "#We initialize the logistic regression class\n",
    "logistic = LogisticRegression() \n",
    "\n",
    "y_tr = y_tr.reshape((len(X_tr),1))\n",
    "y_te = y_te.reshape((len(X_te),1))\n",
    "#We fit the model using a learning rate of 0.01 and 500 iterations\n",
    "values = logistic.fit(X_tr,y_tr, 0.01, 500)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f98b653-f072-4507-a59c-8de4463f6126",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss('Evolution of the loss function, alpha = 0.1, iter = 500', values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658dd98b-3a46-4347-bbc2-3866ef487b08",
   "metadata": {},
   "source": [
    "Now, we estimate labels for the training and the testing dataset. Then we will assess the performance using the [F1-score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdbd39f-b93e-4035-a0c9-2c80064abcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = logistic.predict(X_tr)\n",
    "y_test_pred = logistic.predict(X_te)\n",
    "\n",
    "print(f'Performance in the training set:{f1_score(y_tr, y_train_pred)}\\n')\n",
    "print(f'Performance in the test set:{f1_score(y_te, y_test_pred)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedab9d2-c8f1-4f85-abc6-f8aca5db5ff2",
   "metadata": {},
   "source": [
    "How does your model perform? Are you satisfied? Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af98bfd-f7fe-4396-a45b-2d6664bc1bf0",
   "metadata": {},
   "source": [
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefabe52-1e99-4112-9ba6-cabd8f750e5b",
   "metadata": {},
   "source": [
    "### Task 2.2: Varying the learning rate and the number of iterations\n",
    "Run multiple times the fit function, using different values of the learning rate (0.001 and 0.1) and the iterations (500 and 1000). Comment on your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd3eae-c560-41f0-abf4-c6fae389b7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f99fee9-1bb5-4013-9588-656ac7b37ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c476fe-da05-4e6f-b811-d5c49973169a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a671ce53-26f5-4f98-952c-e0ed29e1d1fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d2b6ae8-68a1-412b-9e1d-3b6bedeab090",
   "metadata": {},
   "source": [
    "Comment on the obtained curves. How does the behavior of the loss changes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e453c5-ee86-4cfe-9e75-34961ed39887",
   "metadata": {},
   "source": [
    "Your answer here: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554090ef-778e-4064-832b-9f0b3143b3b8",
   "metadata": {},
   "source": [
    "## Optional Exercise: Changing the Properties of the Data (Bonus point)\n",
    "Play around with the make_classification function by varying the number of redundant, repeated and informative features. For each new dataset you generate, train the logistic regression classifier. Comment on the results you obtained. What happens when there are too many redundant and/or repeated features? Too many random ones? How does the number of informative features affect the quality of the classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce18b979-f4dc-45d4-b738-949a7fb29aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
