{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28a262a0-0820-4b5d-9ddc-307033049906",
   "metadata": {},
   "source": [
    "## Part II – Using the perceptron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df4098fa-7883-49cb-a029-cacd849c54b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "\n",
    "using Flux, Images, MLDatasets, Plots\n",
    "\n",
    "using Flux: crossentropy, onecold, onehotbatch, params, train!\n",
    "\n",
    "using Random, Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "727cfc19-7678-438d-ad62-8710f1a9af9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskLocalRNG()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set random seed\n",
    "\n",
    "Random.seed!(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dafbbafc-3003-4892-bef6-81f19a500598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(typeof(X_trainᵨ), size(X_trainᵨ)) = (Array{Float32, 3}, (28, 28, 60000))\n",
      "(typeof(X_testᵨ), size(X_testᵨ)) = (Array{Float32, 3}, (28, 28, 10000))\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "# Latex: let's use \\_rho for Raw Data i.e X_trainᵨ\n",
    "# X contains the images and y contains the labels\n",
    "\n",
    "X_trainᵨ, y_trainᵨ = MLDatasets.MNIST(:train)[:]; @show typeof(X_trainᵨ), size(X_trainᵨ)\n",
    "\n",
    "X_testᵨ, y_testᵨ = MLDatasets.MNIST(:test)[:]; @show typeof(X_testᵨ), size(X_testᵨ);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be3cfb9e-3292-4216-ba0d-b5b4f3099415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAiVJREFUaAW9wT2IFgQABuAHemnI4aJFISgJwkDIIiqIsMLcajiKIEEIshosmgSHhhoUIW/IcIgCISHa+psKsp8hEKSSSAlyECon61Q+jMDT4RuO7w6/n5Pe54myKIuyKIuyKIuyKIuyKIuyKIuyKIuyKIuyKIuyKIsZ3YQ5y17FLdiE3TiI5/EvDuAto6IsyqIsJrgDN+MRPIpb8YzV/sAhzOMSTuI7q0VZlEVZjHE/vsac8ZbwBgb4CH/hH/xmtSiLsiiLMc7iPOasdhyLeAL/4ajpRFmURVmM8Tf24Cn8hEOGfsZ2DLAZr5telEVZlMUEn+IYLmELXsQCBoZ+xcumF2VRFmUxhYuGLhjahY+xZHZRFmVRFjN4Ew/gMTyJr8wuyqIsymIGA7yEH/E+vsEJHMZV04myKIuymNEZvIAj2ImdWIcPcc5kURZlURZr8Al+xwK2YT/uxD78abwoi7IoizX6Bc/haRzBK7gb240XZVEWZXEDFnEUHyDYisfxreuLsiiLslije/EsHkQMncL3xouyKIuymNEmvIZ5bLDsCs5hyXhRFmVRFlPagB3YjY1GncA+fG6yKIuyKIsJ1mMz3sU9Rh3H2/gMS6YTZVEWZXEdt+E93Ie7jPoBC/gSl80myqIsymKFh7EHD+F2oy7jHezHwNpEWZRFWawwj3nLTuMLXMFBLLoxURZlURYr7MVe/58oi7Ioi7Ioi7Ioi7Ioi7IouwZsVVgTmd3ynQAAAABJRU5ErkJggg==",
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAiVJREFUaAW9wT2IFgQABuAHemnI4aJFISgJwkDIIiqIsMLcajiKIEEIshosmgSHhhoUIW/IcIgCISHa+psKsp8hEKSSSAlyECon61Q+jMDT4RuO7w6/n5Pe54myKIuyKIuyKIuyKIuyKIuyKIuyKIuyKIuyKIuyKIsZ3YQ5y17FLdiE3TiI5/EvDuAto6IsyqIsJrgDN+MRPIpb8YzV/sAhzOMSTuI7q0VZlEVZjHE/vsac8ZbwBgb4CH/hH/xmtSiLsiiLMc7iPOasdhyLeAL/4ajpRFmURVmM8Tf24Cn8hEOGfsZ2DLAZr5telEVZlMUEn+IYLmELXsQCBoZ+xcumF2VRFmUxhYuGLhjahY+xZHZRFmVRFjN4Ew/gMTyJr8wuyqIsymIGA7yEH/E+vsEJHMZV04myKIuymNEZvIAj2ImdWIcPcc5kURZlURZr8Al+xwK2YT/uxD78abwoi7IoizX6Bc/haRzBK7gb240XZVEWZXEDFnEUHyDYisfxreuLsiiLslije/EsHkQMncL3xouyKIuymNEmvIZ5bLDsCs5hyXhRFmVRFlPagB3YjY1GncA+fG6yKIuyKIsJ1mMz3sU9Rh3H2/gMS6YTZVEWZXEdt+E93Ie7jPoBC/gSl80myqIsymKFh7EHD+F2oy7jHezHwNpEWZRFWawwj3nLTuMLXMFBLLoxURZlURYr7MVe/58oi7Ioi7Ioi7Ioi7Ioi7IouwZsVVgTmd3ynQAAAABJRU5ErkJg\">"
      ],
      "text/plain": [
       "28×28 reinterpret(reshape, Gray{Float32}, adjoint(::Matrix{Float32})) with eltype Gray{Float32}:\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " ⋮                                       ⋱  \n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view training input\n",
    "\n",
    "index = 1; img = X_trainᵨ[:, :, index]\n",
    "\n",
    "# use the ' transpose sign to invert the image\n",
    "\n",
    "colorview(Gray, img')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07c864c0-513a-4ca4-98f0-073b627cbff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view training label\n",
    "\n",
    "y_trainᵨ[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c82959c-83ef-4f81-9f72-778fba2b1f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAdBJREFUaAW9wb1qlgcABtCDeToUXLRU6FB/cOtSgggFWyh0EV0EvYXUoXQpBFxCQYdAxg7egeAFlBIKKXTRJYv4UyrGwYoIhQS0Q1ChDu8QBL/4vfnCc06URVmURVmURVmURVmURVmURVmURVmURVmURVmURVmURVmURVmURVmURVlMcAkLeIZt3MBzPDKbKIuyKIsJVnDcjst4ift29xQrWPd+URZlURYTLOBLPMAXmMe3+Ar/4HM73uBffGbwBOveL8qiLMpigjWsGawaHMI81nHajm08xF84jMcmi7Ioi7IYYQt/GKx510Ucwl3cNFmURVmUxT44gus4gKvYNFmURVmUxT74AZ9iC3/bXZRFWZTFjM7gisEF3LO7KIuyKIsZncNHWMNtHxZlURZlMYOPcRav8DNe+7Aoi7IoixksYh6ruGU6URZlURZ7dB5LeIFrphdlURZlsQef4BfM4TfcNr0oi7Ioi5HmsIoT2MCScaIsyqIsRjqJUwY/YcM4URZlURYjHMPvBov41XhRFmVRFiN8j6MGf+J/40VZlEVZTOkb/Gh2URZlURZT+hoHDTbwn72JsiiLshjpDr7Dpr2JsiiLspjSMpbNLsqiLMreApamPWWOWvFrAAAAAElFTkSuQmCC",
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAdBJREFUaAW9wb1qlgcABtCDeToUXLRU6FB/cOtSgggFWyh0EV0EvYXUoXQpBFxCQYdAxg7egeAFlBIKKXTRJYv4UyrGwYoIhQS0Q1ChDu8QBL/4vfnCc06URVmURVmURVmURVmURVmURVmURVmURVmURVmURVmURVmURVmURVmURVlMcAkLeIZt3MBzPDKbKIuyKIsJVnDcjst4ift29xQrWPd+URZlURYTLOBLPMAXmMe3+Ar/4HM73uBffGbwBOveL8qiLMpigjWsGawaHMI81nHajm08xF84jMcmi7Ioi7IYYQt/GKx510Ucwl3cNFmURVmUxT44gus4gKvYNFmURVmUxT74AZ9iC3/bXZRFWZTFjM7gisEF3LO7KIuyKIsZncNHWMNtHxZlURZlMYOPcRav8DNe+7Aoi7IoixksYh6ruGU6URZlURZ7dB5LeIFrphdlURZlsQef4BfM4TfcNr0oi7Ioi5HmsIoT2MCScaIsyqIsRjqJUwY/YcM4URZlURYjHMPvBov41XhRFmVRFiN8j6MGf+J/40VZlEVZTOkb/Gh2URZlURZT+hoHDTbwn72JsiiLshjpDr7Dpr2JsiiLspjSMpbNLsqiLMreApamPWWOWvFrAAAAAElFTkSuQmCC\">"
      ],
      "text/plain": [
       "28×28 reinterpret(reshape, Gray{Float32}, adjoint(::Matrix{Float32})) with eltype Gray{Float32}:\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " ⋮                                       ⋱  \n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view testing input\n",
    "\n",
    "colorview(Gray, X_testᵨ[:, :, index]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b22da92-8ae8-4254-841f-cd9acf722b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view testing label\n",
    "\n",
    "y_testᵨ[index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e6b15e-4eb3-4751-8eef-3d6ba00621d1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f309f725-a39d-42eb-bac1-7927f808b0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(typeof(X_train), size(X_train)) = (Matrix{Float32}, (784, 60000))\n",
      "(typeof(X_test), size(X_test)) = (Matrix{Float32}, (784, 10000))\n"
     ]
    }
   ],
   "source": [
    "# flatten input data\n",
    "\n",
    "X_train = Flux.flatten(X_trainᵨ); @show typeof(X_train), size(X_train)\n",
    "\n",
    "X_test = Flux.flatten(X_testᵨ); @show typeof(X_test), size(X_test);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de32df7c-8e8d-46ba-9d99-79baf2edda86",
   "metadata": {},
   "source": [
    "&#x1F4CD; Note: Dimensions\n",
    "\n",
    "> Hint: The digits dataset is a dataset of images. You need to convert them from a 2D array to a 1D one.\n",
    "\n",
    "Flattening allow the data to strip down from an array of three dimensions `Array{Float32, 3}, (28, 28, 60000)` to a Matrix  `Matrix{Float32}, (784, 60000)`.\n",
    "The 28x28 tensor array has been replaced to a 784 column vector where each column contains the floating point numbers associated with each image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7200787b-8163-4023-9561-1ece75d5647e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×10000 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n",
       " ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  …  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  1  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  1  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  …  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅     ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1\n",
       " 1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  1  ⋅  ⋅  1     ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encode labels\n",
    "\n",
    "y_train = onehotbatch(y_trainᵨ, 0:9)\n",
    "\n",
    "y_test = onehotbatch(y_testᵨ, 0:9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7416ac1e-3acc-4242-b576-59776d69095c",
   "metadata": {},
   "source": [
    "&#x1F4CD; Note: one-hot encoding\n",
    "\n",
    "As you can see with the above result,  the 1st label appearing in `y_testᵨ` previously viewed is a `seven` label. In the column vector, it appears at the 8th row; (marked by  a ${\\color{Green}1}$) because, the column vector starts from 0 and ends at 9 (labels). The dots represent  zeroes ${\\color{Salmon}0}$.\n",
    "\n",
    "\n",
    "As a side note, Julia' indexing is 1-based, like Matlab. Python indexing is 0-based."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb69e9b-f9e8-4549-8242-fac3817f5ce1",
   "metadata": {},
   "source": [
    "<img src=images/one-hot-label.png width='' height='' > </img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2724698f-7926-4871-969c-ce054c7b9579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(784 => 32, relu),               \u001b[90m# 25_120 parameters\u001b[39m\n",
       "  Dense(32 => 10),                      \u001b[90m# 330 parameters\u001b[39m\n",
       "  NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m25_450 parameters, 99.664 KiB."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model architecture\n",
    "\n",
    "model = Chain(\n",
    "    Dense(28 * 28, 32, relu),\n",
    "    Dense(32, 10),\n",
    "    softmax\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18709d59-2de4-4997-acdc-24c1f657f6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "𝐿 (generic function with 1 method)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define loss function 𝐿\n",
    "\n",
    "𝐿(x, y) = crossentropy(model(x), y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f53b4f55-cb9a-4e21-acad-2fbe66e7751d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# track parameters called θ\n",
    "\n",
    "θ = params(model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3d04448-a8ac-4c9d-9f99-f51c55fd80c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam(0.009999999776482582, (0.9, 0.999), 1.0e-8, IdDict{Any, Any}())"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select optimizer\n",
    "\n",
    "# Let's define the learning rate α\n",
    "α = Float32(0.01)\n",
    "\n",
    "opt = ADAM(α)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7930235-7001-44f7-a66d-65a46b8a1a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1 : Training Loss = 1.9509032\n",
      "Epoch = 2 : Training Loss = 1.5912675\n",
      "Epoch = 3 : Training Loss = 1.3159828\n",
      "Epoch = 4 : Training Loss = 1.0808469\n",
      "Epoch = 5 : Training Loss = 0.9106167\n",
      "Epoch = 6 : Training Loss = 0.7821645\n",
      "Epoch = 7 : Training Loss = 0.68059564\n",
      "Epoch = 8 : Training Loss = 0.60581094\n",
      "Epoch = 9 : Training Loss = 0.5507695\n",
      "Epoch = 10 : Training Loss = 0.5092736\n",
      "Epoch = 11 : Training Loss = 0.4742574\n",
      "Epoch = 12 : Training Loss = 0.44711536\n",
      "Epoch = 13 : Training Loss = 0.43146166\n",
      "Epoch = 14 : Training Loss = 0.4172105\n",
      "Epoch = 15 : Training Loss = 0.40199247\n",
      "Epoch = 16 : Training Loss = 0.3903303\n",
      "Epoch = 17 : Training Loss = 0.38151392\n",
      "Epoch = 18 : Training Loss = 0.37256595\n",
      "Epoch = 19 : Training Loss = 0.36369956\n",
      "Epoch = 20 : Training Loss = 0.35531312\n",
      "Epoch = 21 : Training Loss = 0.3473193\n",
      "Epoch = 22 : Training Loss = 0.34133586\n",
      "Epoch = 23 : Training Loss = 0.3364228\n",
      "Epoch = 24 : Training Loss = 0.33075646\n",
      "Epoch = 25 : Training Loss = 0.32532123\n",
      "Epoch = 26 : Training Loss = 0.32085887\n",
      "Epoch = 27 : Training Loss = 0.3168327\n",
      "Epoch = 28 : Training Loss = 0.31277087\n",
      "Epoch = 29 : Training Loss = 0.308859\n",
      "Epoch = 30 : Training Loss = 0.30496353\n",
      "Epoch = 31 : Training Loss = 0.3008776\n",
      "Epoch = 32 : Training Loss = 0.2973187\n",
      "Epoch = 33 : Training Loss = 0.2941134\n",
      "Epoch = 34 : Training Loss = 0.2906521\n",
      "Epoch = 35 : Training Loss = 0.2873051\n",
      "Epoch = 36 : Training Loss = 0.2841263\n",
      "Epoch = 37 : Training Loss = 0.28094578\n",
      "Epoch = 38 : Training Loss = 0.27800515\n",
      "Epoch = 39 : Training Loss = 0.2752022\n",
      "Epoch = 40 : Training Loss = 0.27228808\n",
      "Epoch = 41 : Training Loss = 0.2694363\n",
      "Epoch = 42 : Training Loss = 0.2668066\n",
      "Epoch = 43 : Training Loss = 0.26422974\n",
      "Epoch = 44 : Training Loss = 0.26172167\n",
      "Epoch = 45 : Training Loss = 0.25932282\n",
      "Epoch = 46 : Training Loss = 0.25690648\n",
      "Epoch = 47 : Training Loss = 0.25457418\n",
      "Epoch = 48 : Training Loss = 0.252339\n",
      "Epoch = 49 : Training Loss = 0.25009468\n",
      "Epoch = 50 : Training Loss = 0.24787965\n",
      "Epoch = 51 : Training Loss = 0.24574046\n",
      "Epoch = 52 : Training Loss = 0.24362005\n",
      "Epoch = 53 : Training Loss = 0.24152513\n",
      "Epoch = 54 : Training Loss = 0.23949514\n",
      "Epoch = 55 : Training Loss = 0.23745945\n",
      "Epoch = 56 : Training Loss = 0.23546022\n",
      "Epoch = 57 : Training Loss = 0.23349476\n",
      "Epoch = 58 : Training Loss = 0.23151833\n",
      "Epoch = 59 : Training Loss = 0.22959527\n",
      "Epoch = 60 : Training Loss = 0.22771133\n",
      "Epoch = 61 : Training Loss = 0.22586778\n",
      "Epoch = 62 : Training Loss = 0.22406754\n",
      "Epoch = 63 : Training Loss = 0.22229847\n",
      "Epoch = 64 : Training Loss = 0.22055936\n",
      "Epoch = 65 : Training Loss = 0.21886593\n",
      "Epoch = 66 : Training Loss = 0.2171879\n",
      "Epoch = 67 : Training Loss = 0.21552514\n",
      "Epoch = 68 : Training Loss = 0.21388555\n",
      "Epoch = 69 : Training Loss = 0.21226807\n",
      "Epoch = 70 : Training Loss = 0.2106775\n",
      "Epoch = 71 : Training Loss = 0.20910269\n",
      "Epoch = 72 : Training Loss = 0.20754363\n",
      "Epoch = 73 : Training Loss = 0.20600566\n",
      "Epoch = 74 : Training Loss = 0.20449232\n",
      "Epoch = 75 : Training Loss = 0.20299606\n",
      "Epoch = 76 : Training Loss = 0.20151354\n",
      "Epoch = 77 : Training Loss = 0.20004384\n",
      "Epoch = 78 : Training Loss = 0.19859317\n",
      "Epoch = 79 : Training Loss = 0.19716525\n",
      "Epoch = 80 : Training Loss = 0.19575107\n",
      "Epoch = 81 : Training Loss = 0.19435064\n",
      "Epoch = 82 : Training Loss = 0.19296654\n",
      "Epoch = 83 : Training Loss = 0.19160365\n",
      "Epoch = 84 : Training Loss = 0.19025762\n",
      "Epoch = 85 : Training Loss = 0.1889262\n",
      "Epoch = 86 : Training Loss = 0.18760784\n",
      "Epoch = 87 : Training Loss = 0.1863058\n",
      "Epoch = 88 : Training Loss = 0.18502161\n",
      "Epoch = 89 : Training Loss = 0.1837555\n",
      "Epoch = 90 : Training Loss = 0.18250395\n",
      "Epoch = 91 : Training Loss = 0.1812723\n",
      "Epoch = 92 : Training Loss = 0.18005566\n",
      "Epoch = 93 : Training Loss = 0.17885739\n",
      "Epoch = 94 : Training Loss = 0.17767614\n",
      "Epoch = 95 : Training Loss = 0.17651036\n",
      "Epoch = 96 : Training Loss = 0.17535989\n",
      "Epoch = 97 : Training Loss = 0.1742239\n",
      "Epoch = 98 : Training Loss = 0.17310351\n",
      "Epoch = 99 : Training Loss = 0.17200191\n",
      "Epoch = 100 : Training Loss = 0.17091592\n",
      "Epoch = 101 : Training Loss = 0.16984485\n",
      "Epoch = 102 : Training Loss = 0.1687915\n",
      "Epoch = 103 : Training Loss = 0.16775446\n",
      "Epoch = 104 : Training Loss = 0.16673233\n",
      "Epoch = 105 : Training Loss = 0.1657241\n",
      "Epoch = 106 : Training Loss = 0.16472802\n",
      "Epoch = 107 : Training Loss = 0.16374353\n",
      "Epoch = 108 : Training Loss = 0.1627726\n",
      "Epoch = 109 : Training Loss = 0.16181105\n",
      "Epoch = 110 : Training Loss = 0.16085993\n",
      "Epoch = 111 : Training Loss = 0.15991922\n",
      "Epoch = 112 : Training Loss = 0.15898998\n",
      "Epoch = 113 : Training Loss = 0.15806937\n",
      "Epoch = 114 : Training Loss = 0.15715769\n",
      "Epoch = 115 : Training Loss = 0.15625933\n",
      "Epoch = 116 : Training Loss = 0.1553714\n",
      "Epoch = 117 : Training Loss = 0.15449372\n",
      "Epoch = 118 : Training Loss = 0.15362881\n",
      "Epoch = 119 : Training Loss = 0.15277259\n",
      "Epoch = 120 : Training Loss = 0.1519254\n",
      "Epoch = 121 : Training Loss = 0.15108828\n",
      "Epoch = 122 : Training Loss = 0.15026157\n",
      "Epoch = 123 : Training Loss = 0.14944549\n",
      "Epoch = 124 : Training Loss = 0.14863747\n",
      "Epoch = 125 : Training Loss = 0.14783683\n",
      "Epoch = 126 : Training Loss = 0.14704324\n",
      "Epoch = 127 : Training Loss = 0.14625883\n",
      "Epoch = 128 : Training Loss = 0.14548399\n",
      "Epoch = 129 : Training Loss = 0.14471799\n",
      "Epoch = 130 : Training Loss = 0.143961\n",
      "Epoch = 131 : Training Loss = 0.14321063\n",
      "Epoch = 132 : Training Loss = 0.14246719\n",
      "Epoch = 133 : Training Loss = 0.14173104\n",
      "Epoch = 134 : Training Loss = 0.14100274\n",
      "Epoch = 135 : Training Loss = 0.14028105\n",
      "Epoch = 136 : Training Loss = 0.13956547\n",
      "Epoch = 137 : Training Loss = 0.13885911\n",
      "Epoch = 138 : Training Loss = 0.13816094\n",
      "Epoch = 139 : Training Loss = 0.13747022\n",
      "Epoch = 140 : Training Loss = 0.13678603\n",
      "Epoch = 141 : Training Loss = 0.13610807\n",
      "Epoch = 142 : Training Loss = 0.13543606\n",
      "Epoch = 143 : Training Loss = 0.13476813\n",
      "Epoch = 144 : Training Loss = 0.1341052\n",
      "Epoch = 145 : Training Loss = 0.13344736\n",
      "Epoch = 146 : Training Loss = 0.13279386\n",
      "Epoch = 147 : Training Loss = 0.13214408\n",
      "Epoch = 148 : Training Loss = 0.13149738\n",
      "Epoch = 149 : Training Loss = 0.1308566\n",
      "Epoch = 150 : Training Loss = 0.13022122\n",
      "Epoch = 151 : Training Loss = 0.12959021\n",
      "Epoch = 152 : Training Loss = 0.1289628\n",
      "Epoch = 153 : Training Loss = 0.12834117\n",
      "Epoch = 154 : Training Loss = 0.12772347\n",
      "Epoch = 155 : Training Loss = 0.12710913\n",
      "Epoch = 156 : Training Loss = 0.12649897\n",
      "Epoch = 157 : Training Loss = 0.1258949\n",
      "Epoch = 158 : Training Loss = 0.12529668\n",
      "Epoch = 159 : Training Loss = 0.124703586\n",
      "Epoch = 160 : Training Loss = 0.124115705\n",
      "Epoch = 161 : Training Loss = 0.123533145\n",
      "Epoch = 162 : Training Loss = 0.12295575\n",
      "Epoch = 163 : Training Loss = 0.12238295\n",
      "Epoch = 164 : Training Loss = 0.121815264\n",
      "Epoch = 165 : Training Loss = 0.12125231\n",
      "Epoch = 166 : Training Loss = 0.12069279\n",
      "Epoch = 167 : Training Loss = 0.120137304\n",
      "Epoch = 168 : Training Loss = 0.11958611\n",
      "Epoch = 169 : Training Loss = 0.11903895\n",
      "Epoch = 170 : Training Loss = 0.118495986\n",
      "Epoch = 171 : Training Loss = 0.1179558\n",
      "Epoch = 172 : Training Loss = 0.117418736\n",
      "Epoch = 173 : Training Loss = 0.11688501\n",
      "Epoch = 174 : Training Loss = 0.11635503\n",
      "Epoch = 175 : Training Loss = 0.11582886\n",
      "Epoch = 176 : Training Loss = 0.11530721\n",
      "Epoch = 177 : Training Loss = 0.11478739\n",
      "Epoch = 178 : Training Loss = 0.114271306\n",
      "Epoch = 179 : Training Loss = 0.113758095\n",
      "Epoch = 180 : Training Loss = 0.113248356\n",
      "Epoch = 181 : Training Loss = 0.11274338\n",
      "Epoch = 182 : Training Loss = 0.112240136\n",
      "Epoch = 183 : Training Loss = 0.11173938\n",
      "Epoch = 184 : Training Loss = 0.11124209\n",
      "Epoch = 185 : Training Loss = 0.1107499\n",
      "Epoch = 186 : Training Loss = 0.110260904\n",
      "Epoch = 187 : Training Loss = 0.109775424\n",
      "Epoch = 188 : Training Loss = 0.109292924\n",
      "Epoch = 189 : Training Loss = 0.10881312\n",
      "Epoch = 190 : Training Loss = 0.108335726\n",
      "Epoch = 191 : Training Loss = 0.10786296\n",
      "Epoch = 192 : Training Loss = 0.10739339\n",
      "Epoch = 193 : Training Loss = 0.10692785\n",
      "Epoch = 194 : Training Loss = 0.10646506\n",
      "Epoch = 195 : Training Loss = 0.10600618\n",
      "Epoch = 196 : Training Loss = 0.105550565\n",
      "Epoch = 197 : Training Loss = 0.10509744\n",
      "Epoch = 198 : Training Loss = 0.10464778\n",
      "Epoch = 199 : Training Loss = 0.10420166\n",
      "Epoch = 200 : Training Loss = 0.103758514\n",
      "Epoch = 201 : Training Loss = 0.10331849\n",
      "Epoch = 202 : Training Loss = 0.10288201\n",
      "Epoch = 203 : Training Loss = 0.10244888\n",
      "Epoch = 204 : Training Loss = 0.10201919\n",
      "Epoch = 205 : Training Loss = 0.10159289\n",
      "Epoch = 206 : Training Loss = 0.10116989\n",
      "Epoch = 207 : Training Loss = 0.100749776\n",
      "Epoch = 208 : Training Loss = 0.10033239\n",
      "Epoch = 209 : Training Loss = 0.09991734\n",
      "Epoch = 210 : Training Loss = 0.09950355\n",
      "Epoch = 211 : Training Loss = 0.09909132\n",
      "Epoch = 212 : Training Loss = 0.098681465\n",
      "Epoch = 213 : Training Loss = 0.0982755\n",
      "Epoch = 214 : Training Loss = 0.09787152\n",
      "Epoch = 215 : Training Loss = 0.097469985\n",
      "Epoch = 216 : Training Loss = 0.09707171\n",
      "Epoch = 217 : Training Loss = 0.09667577\n",
      "Epoch = 218 : Training Loss = 0.09628181\n",
      "Epoch = 219 : Training Loss = 0.09589144\n",
      "Epoch = 220 : Training Loss = 0.09550405\n",
      "Epoch = 221 : Training Loss = 0.09511991\n",
      "Epoch = 222 : Training Loss = 0.09473902\n",
      "Epoch = 223 : Training Loss = 0.09436061\n",
      "Epoch = 224 : Training Loss = 0.09398471\n",
      "Epoch = 225 : Training Loss = 0.09361162\n",
      "Epoch = 226 : Training Loss = 0.09324128\n",
      "Epoch = 227 : Training Loss = 0.09287332\n",
      "Epoch = 228 : Training Loss = 0.09250887\n",
      "Epoch = 229 : Training Loss = 0.09214592\n",
      "Epoch = 230 : Training Loss = 0.09178526\n",
      "Epoch = 231 : Training Loss = 0.0914269\n",
      "Epoch = 232 : Training Loss = 0.09107071\n",
      "Epoch = 233 : Training Loss = 0.09071634\n",
      "Epoch = 234 : Training Loss = 0.09036536\n",
      "Epoch = 235 : Training Loss = 0.09001634\n",
      "Epoch = 236 : Training Loss = 0.08967016\n",
      "Epoch = 237 : Training Loss = 0.0893258\n",
      "Epoch = 238 : Training Loss = 0.08898389\n",
      "Epoch = 239 : Training Loss = 0.08864425\n",
      "Epoch = 240 : Training Loss = 0.08830634\n",
      "Epoch = 241 : Training Loss = 0.08797088\n",
      "Epoch = 242 : Training Loss = 0.08763737\n",
      "Epoch = 243 : Training Loss = 0.08730575\n",
      "Epoch = 244 : Training Loss = 0.08697675\n",
      "Epoch = 245 : Training Loss = 0.086649135\n",
      "Epoch = 246 : Training Loss = 0.08632376\n",
      "Epoch = 247 : Training Loss = 0.08600078\n",
      "Epoch = 248 : Training Loss = 0.08567962\n",
      "Epoch = 249 : Training Loss = 0.08536076\n",
      "Epoch = 250 : Training Loss = 0.08504363\n",
      "Epoch = 251 : Training Loss = 0.08472767\n",
      "Epoch = 252 : Training Loss = 0.084414124\n",
      "Epoch = 253 : Training Loss = 0.08410236\n",
      "Epoch = 254 : Training Loss = 0.08379201\n",
      "Epoch = 255 : Training Loss = 0.08348346\n",
      "Epoch = 256 : Training Loss = 0.08317612\n",
      "Epoch = 257 : Training Loss = 0.082871094\n",
      "Epoch = 258 : Training Loss = 0.0825678\n",
      "Epoch = 259 : Training Loss = 0.08226613\n",
      "Epoch = 260 : Training Loss = 0.08196659\n",
      "Epoch = 261 : Training Loss = 0.0816678\n",
      "Epoch = 262 : Training Loss = 0.0813714\n",
      "Epoch = 263 : Training Loss = 0.08107583\n",
      "Epoch = 264 : Training Loss = 0.080782086\n",
      "Epoch = 265 : Training Loss = 0.080490746\n",
      "Epoch = 266 : Training Loss = 0.0802015\n",
      "Epoch = 267 : Training Loss = 0.07991423\n",
      "Epoch = 268 : Training Loss = 0.07962921\n",
      "Epoch = 269 : Training Loss = 0.07934534\n",
      "Epoch = 270 : Training Loss = 0.07906243\n",
      "Epoch = 271 : Training Loss = 0.078781426\n",
      "Epoch = 272 : Training Loss = 0.07850215\n",
      "Epoch = 273 : Training Loss = 0.07822402\n",
      "Epoch = 274 : Training Loss = 0.07794676\n",
      "Epoch = 275 : Training Loss = 0.07767073\n",
      "Epoch = 276 : Training Loss = 0.07739619\n",
      "Epoch = 277 : Training Loss = 0.07712344\n",
      "Epoch = 278 : Training Loss = 0.076852\n",
      "Epoch = 279 : Training Loss = 0.07658213\n",
      "Epoch = 280 : Training Loss = 0.0763131\n",
      "Epoch = 281 : Training Loss = 0.076045215\n",
      "Epoch = 282 : Training Loss = 0.07577829\n",
      "Epoch = 283 : Training Loss = 0.07551314\n",
      "Epoch = 284 : Training Loss = 0.07524879\n",
      "Epoch = 285 : Training Loss = 0.07498641\n",
      "Epoch = 286 : Training Loss = 0.07472535\n",
      "Epoch = 287 : Training Loss = 0.07446579\n",
      "Epoch = 288 : Training Loss = 0.07420842\n",
      "Epoch = 289 : Training Loss = 0.07395249\n",
      "Epoch = 290 : Training Loss = 0.07369659\n",
      "Epoch = 291 : Training Loss = 0.07344204\n",
      "Epoch = 292 : Training Loss = 0.07318877\n",
      "Epoch = 293 : Training Loss = 0.072936445\n",
      "Epoch = 294 : Training Loss = 0.07268508\n",
      "Epoch = 295 : Training Loss = 0.07243386\n",
      "Epoch = 296 : Training Loss = 0.07218429\n",
      "Epoch = 297 : Training Loss = 0.07193597\n",
      "Epoch = 298 : Training Loss = 0.071688466\n",
      "Epoch = 299 : Training Loss = 0.07144204\n",
      "Epoch = 300 : Training Loss = 0.07119719\n",
      "Epoch = 301 : Training Loss = 0.07095351\n",
      "Epoch = 302 : Training Loss = 0.070710376\n",
      "Epoch = 303 : Training Loss = 0.07046753\n",
      "Epoch = 304 : Training Loss = 0.07022583\n",
      "Epoch = 305 : Training Loss = 0.069984816\n",
      "Epoch = 306 : Training Loss = 0.06974483\n",
      "Epoch = 307 : Training Loss = 0.06950627\n",
      "Epoch = 308 : Training Loss = 0.06926882\n",
      "Epoch = 309 : Training Loss = 0.06903134\n",
      "Epoch = 310 : Training Loss = 0.06879466\n",
      "Epoch = 311 : Training Loss = 0.06855908\n",
      "Epoch = 312 : Training Loss = 0.068323866\n",
      "Epoch = 313 : Training Loss = 0.068089485\n",
      "Epoch = 314 : Training Loss = 0.06785563\n",
      "Epoch = 315 : Training Loss = 0.06762286\n",
      "Epoch = 316 : Training Loss = 0.06739041\n",
      "Epoch = 317 : Training Loss = 0.06715847\n",
      "Epoch = 318 : Training Loss = 0.06692812\n",
      "Epoch = 319 : Training Loss = 0.06669759\n",
      "Epoch = 320 : Training Loss = 0.06646833\n",
      "Epoch = 321 : Training Loss = 0.06623982\n",
      "Epoch = 322 : Training Loss = 0.06601247\n",
      "Epoch = 323 : Training Loss = 0.06578647\n",
      "Epoch = 324 : Training Loss = 0.06556148\n",
      "Epoch = 325 : Training Loss = 0.0653378\n",
      "Epoch = 326 : Training Loss = 0.06511467\n",
      "Epoch = 327 : Training Loss = 0.06489191\n",
      "Epoch = 328 : Training Loss = 0.06467062\n",
      "Epoch = 329 : Training Loss = 0.06444996\n",
      "Epoch = 330 : Training Loss = 0.064230576\n",
      "Epoch = 331 : Training Loss = 0.064011924\n",
      "Epoch = 332 : Training Loss = 0.06379495\n",
      "Epoch = 333 : Training Loss = 0.063580014\n",
      "Epoch = 334 : Training Loss = 0.06336542\n",
      "Epoch = 335 : Training Loss = 0.06315112\n",
      "Epoch = 336 : Training Loss = 0.06293835\n",
      "Epoch = 337 : Training Loss = 0.06272711\n",
      "Epoch = 338 : Training Loss = 0.062517285\n",
      "Epoch = 339 : Training Loss = 0.062307894\n",
      "Epoch = 340 : Training Loss = 0.06209991\n",
      "Epoch = 341 : Training Loss = 0.06189302\n",
      "Epoch = 342 : Training Loss = 0.061687265\n",
      "Epoch = 343 : Training Loss = 0.06148202\n",
      "Epoch = 344 : Training Loss = 0.061277483\n",
      "Epoch = 345 : Training Loss = 0.061073802\n",
      "Epoch = 346 : Training Loss = 0.060871392\n",
      "Epoch = 347 : Training Loss = 0.06067075\n",
      "Epoch = 348 : Training Loss = 0.060470484\n",
      "Epoch = 349 : Training Loss = 0.060271543\n",
      "Epoch = 350 : Training Loss = 0.060073\n",
      "Epoch = 351 : Training Loss = 0.059875682\n",
      "Epoch = 352 : Training Loss = 0.059679434\n",
      "Epoch = 353 : Training Loss = 0.059483666\n",
      "Epoch = 354 : Training Loss = 0.05928856\n",
      "Epoch = 355 : Training Loss = 0.05909462\n",
      "Epoch = 356 : Training Loss = 0.058901854\n",
      "Epoch = 357 : Training Loss = 0.058709297\n",
      "Epoch = 358 : Training Loss = 0.058517784\n",
      "Epoch = 359 : Training Loss = 0.058327667\n",
      "Epoch = 360 : Training Loss = 0.05813759\n",
      "Epoch = 361 : Training Loss = 0.057948176\n",
      "Epoch = 362 : Training Loss = 0.05775925\n",
      "Epoch = 363 : Training Loss = 0.057570174\n",
      "Epoch = 364 : Training Loss = 0.057383496\n",
      "Epoch = 365 : Training Loss = 0.057196997\n",
      "Epoch = 366 : Training Loss = 0.057012692\n",
      "Epoch = 367 : Training Loss = 0.05682827\n",
      "Epoch = 368 : Training Loss = 0.05664409\n",
      "Epoch = 369 : Training Loss = 0.056460597\n",
      "Epoch = 370 : Training Loss = 0.056277994\n",
      "Epoch = 371 : Training Loss = 0.056095988\n",
      "Epoch = 372 : Training Loss = 0.055915102\n",
      "Epoch = 373 : Training Loss = 0.055734783\n",
      "Epoch = 374 : Training Loss = 0.055555634\n",
      "Epoch = 375 : Training Loss = 0.055377066\n",
      "Epoch = 376 : Training Loss = 0.05519973\n",
      "Epoch = 377 : Training Loss = 0.055022355\n",
      "Epoch = 378 : Training Loss = 0.05484666\n",
      "Epoch = 379 : Training Loss = 0.054671355\n",
      "Epoch = 380 : Training Loss = 0.054497242\n",
      "Epoch = 381 : Training Loss = 0.05432367\n",
      "Epoch = 382 : Training Loss = 0.054151263\n",
      "Epoch = 383 : Training Loss = 0.053979702\n",
      "Epoch = 384 : Training Loss = 0.0538089\n",
      "Epoch = 385 : Training Loss = 0.05363877\n",
      "Epoch = 386 : Training Loss = 0.053469952\n",
      "Epoch = 387 : Training Loss = 0.053300202\n",
      "Epoch = 388 : Training Loss = 0.05313159\n",
      "Epoch = 389 : Training Loss = 0.05296343\n",
      "Epoch = 390 : Training Loss = 0.05279576\n",
      "Epoch = 391 : Training Loss = 0.052628744\n",
      "Epoch = 392 : Training Loss = 0.052462295\n",
      "Epoch = 393 : Training Loss = 0.052297045\n",
      "Epoch = 394 : Training Loss = 0.05213164\n",
      "Epoch = 395 : Training Loss = 0.051967252\n",
      "Epoch = 396 : Training Loss = 0.051803835\n",
      "Epoch = 397 : Training Loss = 0.051641963\n",
      "Epoch = 398 : Training Loss = 0.05147913\n",
      "Epoch = 399 : Training Loss = 0.051317576\n",
      "Epoch = 400 : Training Loss = 0.051156897\n",
      "Epoch = 401 : Training Loss = 0.050997607\n",
      "Epoch = 402 : Training Loss = 0.050839648\n",
      "Epoch = 403 : Training Loss = 0.050683286\n",
      "Epoch = 404 : Training Loss = 0.050526068\n",
      "Epoch = 405 : Training Loss = 0.050367463\n",
      "Epoch = 406 : Training Loss = 0.050209172\n",
      "Epoch = 407 : Training Loss = 0.050051227\n",
      "Epoch = 408 : Training Loss = 0.049894646\n",
      "Epoch = 409 : Training Loss = 0.04973986\n",
      "Epoch = 410 : Training Loss = 0.04958653\n",
      "Epoch = 411 : Training Loss = 0.049434032\n",
      "Epoch = 412 : Training Loss = 0.04928208\n",
      "Epoch = 413 : Training Loss = 0.04913036\n",
      "Epoch = 414 : Training Loss = 0.048977565\n",
      "Epoch = 415 : Training Loss = 0.04882489\n",
      "Epoch = 416 : Training Loss = 0.048672006\n",
      "Epoch = 417 : Training Loss = 0.048520874\n",
      "Epoch = 418 : Training Loss = 0.048371077\n",
      "Epoch = 419 : Training Loss = 0.04822226\n",
      "Epoch = 420 : Training Loss = 0.04807534\n",
      "Epoch = 421 : Training Loss = 0.047928568\n",
      "Epoch = 422 : Training Loss = 0.047784366\n",
      "Epoch = 423 : Training Loss = 0.047644213\n",
      "Epoch = 424 : Training Loss = 0.047502726\n",
      "Epoch = 425 : Training Loss = 0.04736147\n",
      "Epoch = 426 : Training Loss = 0.047214314\n",
      "Epoch = 427 : Training Loss = 0.047063086\n",
      "Epoch = 428 : Training Loss = 0.04691124\n",
      "Epoch = 429 : Training Loss = 0.046762224\n",
      "Epoch = 430 : Training Loss = 0.0466156\n",
      "Epoch = 431 : Training Loss = 0.0464724\n",
      "Epoch = 432 : Training Loss = 0.04633239\n",
      "Epoch = 433 : Training Loss = 0.046193726\n",
      "Epoch = 434 : Training Loss = 0.04606079\n",
      "Epoch = 435 : Training Loss = 0.045927156\n",
      "Epoch = 436 : Training Loss = 0.045793243\n",
      "Epoch = 437 : Training Loss = 0.04565201\n",
      "Epoch = 438 : Training Loss = 0.0455065\n",
      "Epoch = 439 : Training Loss = 0.045356005\n",
      "Epoch = 440 : Training Loss = 0.045207944\n",
      "Epoch = 441 : Training Loss = 0.04506347\n",
      "Epoch = 442 : Training Loss = 0.044924136\n",
      "Epoch = 443 : Training Loss = 0.04478875\n",
      "Epoch = 444 : Training Loss = 0.044658512\n",
      "Epoch = 445 : Training Loss = 0.044531398\n",
      "Epoch = 446 : Training Loss = 0.044401523\n",
      "Epoch = 447 : Training Loss = 0.044268295\n",
      "Epoch = 448 : Training Loss = 0.044131674\n",
      "Epoch = 449 : Training Loss = 0.04399392\n",
      "Epoch = 450 : Training Loss = 0.043853134\n",
      "Epoch = 451 : Training Loss = 0.04371167\n",
      "Epoch = 452 : Training Loss = 0.043571234\n",
      "Epoch = 453 : Training Loss = 0.04343429\n",
      "Epoch = 454 : Training Loss = 0.043298174\n",
      "Epoch = 455 : Training Loss = 0.04316431\n",
      "Epoch = 456 : Training Loss = 0.043030843\n",
      "Epoch = 457 : Training Loss = 0.042897087\n",
      "Epoch = 458 : Training Loss = 0.04276621\n",
      "Epoch = 459 : Training Loss = 0.042635825\n",
      "Epoch = 460 : Training Loss = 0.042508695\n",
      "Epoch = 461 : Training Loss = 0.042388305\n",
      "Epoch = 462 : Training Loss = 0.0422719\n",
      "Epoch = 463 : Training Loss = 0.042165093\n",
      "Epoch = 464 : Training Loss = 0.04205318\n",
      "Epoch = 465 : Training Loss = 0.041941624\n",
      "Epoch = 466 : Training Loss = 0.041808352\n",
      "Epoch = 467 : Training Loss = 0.04166955\n",
      "Epoch = 468 : Training Loss = 0.041517347\n",
      "Epoch = 469 : Training Loss = 0.041365862\n",
      "Epoch = 470 : Training Loss = 0.04122144\n",
      "Epoch = 471 : Training Loss = 0.041089237\n",
      "Epoch = 472 : Training Loss = 0.040968545\n",
      "Epoch = 473 : Training Loss = 0.04085785\n",
      "Epoch = 474 : Training Loss = 0.040760856\n",
      "Epoch = 475 : Training Loss = 0.04067032\n",
      "Epoch = 476 : Training Loss = 0.040571973\n",
      "Epoch = 477 : Training Loss = 0.04045494\n",
      "Epoch = 478 : Training Loss = 0.040321697\n",
      "Epoch = 479 : Training Loss = 0.04016839\n",
      "Epoch = 480 : Training Loss = 0.04001372\n",
      "Epoch = 481 : Training Loss = 0.03986455\n",
      "Epoch = 482 : Training Loss = 0.039727587\n",
      "Epoch = 483 : Training Loss = 0.03960301\n",
      "Epoch = 484 : Training Loss = 0.039487448\n",
      "Epoch = 485 : Training Loss = 0.039378628\n",
      "Epoch = 486 : Training Loss = 0.039278775\n",
      "Epoch = 487 : Training Loss = 0.039187834\n",
      "Epoch = 488 : Training Loss = 0.03909246\n",
      "Epoch = 489 : Training Loss = 0.038989443\n",
      "Epoch = 490 : Training Loss = 0.038866576\n",
      "Epoch = 491 : Training Loss = 0.038736116\n",
      "Epoch = 492 : Training Loss = 0.038590092\n",
      "Epoch = 493 : Training Loss = 0.038443547\n",
      "Epoch = 494 : Training Loss = 0.038301002\n",
      "Epoch = 495 : Training Loss = 0.038168047\n",
      "Epoch = 496 : Training Loss = 0.038046855\n",
      "Epoch = 497 : Training Loss = 0.037936103\n",
      "Epoch = 498 : Training Loss = 0.03783273\n",
      "Epoch = 499 : Training Loss = 0.037735708\n",
      "Epoch = 500 : Training Loss = 0.03764631\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "\n",
    "# Let's keep track of the loss history\n",
    "loss_history = []\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "for epoch in 1:epochs\n",
    "    # train model\n",
    "    train!(𝐿, θ, [(X_train, y_train)], opt)\n",
    "    # print report\n",
    "    train_loss = 𝐿(X_train, y_train)\n",
    "    push!(loss_history, train_loss)\n",
    "    println(\"Epoch = $epoch : Training Loss = $train_loss\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e15e82bb-49cf-4a35-a1b4-88e4f65713ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9625"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions\n",
    "\n",
    "# Let's call the predicted variable ŷ (Latex: \\hat{y} or y\\hat)\n",
    "\n",
    "ŷᵨ = model(X_test)\n",
    "\n",
    "ŷ = onecold(ŷᵨ) .- 1\n",
    "\n",
    "y = y_testᵨ\n",
    "\n",
    "mean(ŷ .== y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "850c838b-cd31-4eb5-9524-abb64fe26160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000×4 Matrix{Int64}:\n",
       "     1  7  7  1\n",
       "     2  2  2  1\n",
       "     3  1  1  1\n",
       "     4  0  0  1\n",
       "     5  4  4  1\n",
       "     6  1  1  1\n",
       "     7  4  4  1\n",
       "     8  9  9  1\n",
       "     9  4  5  0\n",
       "    10  9  9  1\n",
       "    11  0  0  1\n",
       "    12  6  6  1\n",
       "    13  9  9  1\n",
       "     ⋮        \n",
       "  9989  5  5  1\n",
       "  9990  6  6  1\n",
       "  9991  7  7  1\n",
       "  9992  8  8  1\n",
       "  9993  9  9  1\n",
       "  9994  0  0  1\n",
       "  9995  1  1  1\n",
       "  9996  2  2  1\n",
       "  9997  3  3  1\n",
       "  9998  4  4  1\n",
       "  9999  5  5  1\n",
       " 10000  6  6  1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display results\n",
    "\n",
    "check = [ŷ[i] == y[i] for i in 1:length(y)]\n",
    "\n",
    "index = collect(1:length(y))\n",
    "\n",
    "check_display = [index ŷ y check]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa1b24c6-5669-49b7-935f-f10c727f52ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(y[misclass_index], ŷ[misclass_index]) = (5, 4)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAhhJREFUaAW9wb+rlQUABuBHe8tAGwJtqtuQuDQYiRBBYLQISRRI5NBYgw2piwZiCkpD/QFBQ8MlB8cgiEtBQ2DRcAkUQxquchv6sTiYECkN33CGvuM5pw7v80RZlEVZlEVZlEVZlEVZlEVZlEVZlEVZlMUSPIoVEzdwHFdwHT+aiLIoi7L4H17GKziA3Sau40lsM3jARJRFWZTFgp7CO3gbD2OLf9tjuiiLsiiLBT2Od033E66aLsqiLMpihp04hm/xJf7CLdzGdqzhCr7HOu7gtumiLMqiLO5jO9awF68ZXMaz2MAKNnHP/KIsyqIspngIF7EXH+ArExsGNy0uyqIsymLEDryHQ/gDH+JPyxFlURZlMeJVnMJNvIBblifKoizKYsTzBuvYtFxRFmVRFiMOGxzE+/gc65YjyqIsymLELtzDNpzBaXyM77CCn3HV4Glcxqb5RFmURVmM+AgnTGzFURw17nd8gzfMFmVRFmUx4hQu4TMET2Cr6XbhME7jvPuLsiiLshhxFz9gj8FLeBBnsd+4LdhntiiLsiiLOXxt8Az24298ik9wHEfML8qiLMpiAWu4gOAt7MYBE7+YLcqiLMpiAddwCa8bvGhwF1/gpNmiLMqiLBZwB8fwCPbhMWxgFWfNJ8qiLMpiQb/iEN7EcziH38wvyqIsyuI/WsWqxUVZlEVZlEVZlEVZlEVZlEVZlEVZlEXZP7tATad0MYkTAAAAAElFTkSuQmCC",
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAhhJREFUaAW9wb+rlQUABuBHe8tAGwJtqtuQuDQYiRBBYLQISRRI5NBYgw2piwZiCkpD/QFBQ8MlB8cgiEtBQ2DRcAkUQxquchv6sTiYECkN33CGvuM5pw7v80RZlEVZlEVZlEVZlEVZlEVZlEVZlEVZlMUSPIoVEzdwHFdwHT+aiLIoi7L4H17GKziA3Sau40lsM3jARJRFWZTFgp7CO3gbD2OLf9tjuiiLsiiLBT2Od033E66aLsqiLMpihp04hm/xJf7CLdzGdqzhCr7HOu7gtumiLMqiLO5jO9awF68ZXMaz2MAKNnHP/KIsyqIspngIF7EXH+ArExsGNy0uyqIsymLEDryHQ/gDH+JPyxFlURZlMeJVnMJNvIBblifKoizKYsTzBuvYtFxRFmVRFiMOGxzE+/gc65YjyqIsymLELtzDNpzBaXyM77CCn3HV4Glcxqb5RFmURVmM+AgnTGzFURw17nd8gzfMFmVRFmUx4hQu4TMET2Cr6XbhME7jvPuLsiiLshhxFz9gj8FLeBBnsd+4LdhntiiLsiiLOXxt8Az24298ik9wHEfML8qiLMpiAWu4gOAt7MYBE7+YLcqiLMpiAddwCa8bvGhwF1/gpNmiLMqiLBZwB8fwCPbhMWxgFWfNJ8qiLMpiQb/iEN7EcziH38wvyqIsyuI/WsWqxUVZlEVZlEVZlEVZlEVZlEVZlEVZlEXZP7tATad0MYkTAAAAAElFTkSuQmCC\">"
      ],
      "text/plain": [
       "28×28 reinterpret(reshape, Gray{Float32}, adjoint(::Matrix{Float32})) with eltype Gray{Float32}:\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " ⋮                                       ⋱  \n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view misclassifications\n",
    "\n",
    "misclass_index = 9\n",
    "img = X_testᵨ[:, :, misclass_index]\n",
    "\n",
    "@show y[misclass_index], ŷ[misclass_index];\n",
    "\n",
    "colorview(Gray, img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d552a462-682f-4e26-a0e1-3e38426ec732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize plot\n",
    "\n",
    "gr(size = (600, 600))\n",
    "\n",
    "# plot learning curve\n",
    "\n",
    "p_l_curve = plot(1:epochs, loss_history,\n",
    "    xlabel = \"Epochs\",\n",
    "    ylabel = \"Loss\",\n",
    "    title = \"Learning Curve\",\n",
    "    legend = false,\n",
    "    color = :blue,\n",
    "    linewidth = 2\n",
    ")\n",
    "# save plot\n",
    "savefig(p_l_curve, \"images/ann_learning_curve.svg\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4bc90f-8967-406a-9559-a8ffc0bec907",
   "metadata": {},
   "source": [
    "<img src=images/ann_learning_curve.svg width='' height='' > </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb76e99-7526-40f6-9c87-791665818371",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [ ] [Is there a way to print loss from Flux.train?](https://stackoverflow.com/questions/73737260/is-there-a-way-to-print-loss-from-flux-train)\n",
    "- [ ] [The Future of Machine Learning and why it looks a lot like Julia 🤖](https://towardsdatascience.com/the-future-of-machine-learning-and-why-it-looks-a-lot-like-julia-a0e26b51f6a6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
