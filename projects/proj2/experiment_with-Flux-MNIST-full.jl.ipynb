{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28a262a0-0820-4b5d-9ddc-307033049906",
   "metadata": {},
   "source": [
    "## Part II – Using the perceptron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df4098fa-7883-49cb-a029-cacd849c54b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "\n",
    "using Flux, Images, MLDatasets, Plots\n",
    "\n",
    "using Flux: crossentropy, onecold, onehotbatch, params, train!\n",
    "\n",
    "using Random, Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "727cfc19-7678-438d-ad62-8710f1a9af9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskLocalRNG()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set random seed\n",
    "\n",
    "Random.seed!(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dafbbafc-3003-4892-bef6-81f19a500598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(typeof(X_trainᵨ), size(X_trainᵨ)) = (Array{Float32, 3}, (28, 28, 60000))\n",
      "(typeof(X_testᵨ), size(X_testᵨ)) = (Array{Float32, 3}, (28, 28, 10000))\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "# Latex: let's use \\_rho for Raw Data i.e X_trainᵨ\n",
    "# X contains the images and y contains the labels\n",
    "\n",
    "X_trainᵨ, y_trainᵨ = MLDatasets.MNIST(:train)[:]; @show typeof(X_trainᵨ), size(X_trainᵨ)\n",
    "\n",
    "X_testᵨ, y_testᵨ = MLDatasets.MNIST(:test)[:]; @show typeof(X_testᵨ), size(X_testᵨ);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be3cfb9e-3292-4216-ba0d-b5b4f3099415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAiVJREFUaAW9wT2IFgQABuAHemnI4aJFISgJwkDIIiqIsMLcajiKIEEIshosmgSHhhoUIW/IcIgCISHa+psKsp8hEKSSSAlyECon61Q+jMDT4RuO7w6/n5Pe54myKIuyKIuyKIuyKIuyKIuyKIuyKIuyKIuyKIuyKIsZ3YQ5y17FLdiE3TiI5/EvDuAto6IsyqIsJrgDN+MRPIpb8YzV/sAhzOMSTuI7q0VZlEVZjHE/vsac8ZbwBgb4CH/hH/xmtSiLsiiLMc7iPOasdhyLeAL/4ajpRFmURVmM8Tf24Cn8hEOGfsZ2DLAZr5telEVZlMUEn+IYLmELXsQCBoZ+xcumF2VRFmUxhYuGLhjahY+xZHZRFmVRFjN4Ew/gMTyJr8wuyqIsymIGA7yEH/E+vsEJHMZV04myKIuymNEZvIAj2ImdWIcPcc5kURZlURZr8Al+xwK2YT/uxD78abwoi7IoizX6Bc/haRzBK7gb240XZVEWZXEDFnEUHyDYisfxreuLsiiLslije/EsHkQMncL3xouyKIuymNEmvIZ5bLDsCs5hyXhRFmVRFlPagB3YjY1GncA+fG6yKIuyKIsJ1mMz3sU9Rh3H2/gMS6YTZVEWZXEdt+E93Ie7jPoBC/gSl80myqIsymKFh7EHD+F2oy7jHezHwNpEWZRFWawwj3nLTuMLXMFBLLoxURZlURYr7MVe/58oi7Ioi7Ioi7Ioi7Ioi7IouwZsVVgTmd3ynQAAAABJRU5ErkJggg==",
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAiVJREFUaAW9wT2IFgQABuAHemnI4aJFISgJwkDIIiqIsMLcajiKIEEIshosmgSHhhoUIW/IcIgCISHa+psKsp8hEKSSSAlyECon61Q+jMDT4RuO7w6/n5Pe54myKIuyKIuyKIuyKIuyKIuyKIuyKIuyKIuyKIuyKIsZ3YQ5y17FLdiE3TiI5/EvDuAto6IsyqIsJrgDN+MRPIpb8YzV/sAhzOMSTuI7q0VZlEVZjHE/vsac8ZbwBgb4CH/hH/xmtSiLsiiLMc7iPOasdhyLeAL/4ajpRFmURVmM8Tf24Cn8hEOGfsZ2DLAZr5telEVZlMUEn+IYLmELXsQCBoZ+xcumF2VRFmUxhYuGLhjahY+xZHZRFmVRFjN4Ew/gMTyJr8wuyqIsymIGA7yEH/E+vsEJHMZV04myKIuymNEZvIAj2ImdWIcPcc5kURZlURZr8Al+xwK2YT/uxD78abwoi7IoizX6Bc/haRzBK7gb240XZVEWZXEDFnEUHyDYisfxreuLsiiLslije/EsHkQMncL3xouyKIuymNEmvIZ5bLDsCs5hyXhRFmVRFlPagB3YjY1GncA+fG6yKIuyKIsJ1mMz3sU9Rh3H2/gMS6YTZVEWZXEdt+E93Ie7jPoBC/gSl80myqIsymKFh7EHD+F2oy7jHezHwNpEWZRFWawwj3nLTuMLXMFBLLoxURZlURYr7MVe/58oi7Ioi7Ioi7Ioi7Ioi7IouwZsVVgTmd3ynQAAAABJRU5ErkJg\">"
      ],
      "text/plain": [
       "28×28 reinterpret(reshape, Gray{Float32}, adjoint(::Matrix{Float32})) with eltype Gray{Float32}:\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " ⋮                                       ⋱  \n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view training input\n",
    "\n",
    "index = 1; img = X_trainᵨ[:, :, index]\n",
    "\n",
    "# use the ' transpose sign to invert the image\n",
    "\n",
    "colorview(Gray, img')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07c864c0-513a-4ca4-98f0-073b627cbff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view training label\n",
    "\n",
    "y_trainᵨ[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c82959c-83ef-4f81-9f72-778fba2b1f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAdBJREFUaAW9wb1qlgcABtCDeToUXLRU6FB/cOtSgggFWyh0EV0EvYXUoXQpBFxCQYdAxg7egeAFlBIKKXTRJYv4UyrGwYoIhQS0Q1ChDu8QBL/4vfnCc06URVmURVmURVmURVmURVmURVmURVmURVmURVmURVmURVmURVmURVmURVlMcAkLeIZt3MBzPDKbKIuyKIsJVnDcjst4ift29xQrWPd+URZlURYTLOBLPMAXmMe3+Ar/4HM73uBffGbwBOveL8qiLMpigjWsGawaHMI81nHajm08xF84jMcmi7Ioi7IYYQt/GKx510Ucwl3cNFmURVmUxT44gus4gKvYNFmURVmUxT74AZ9iC3/bXZRFWZTFjM7gisEF3LO7KIuyKIsZncNHWMNtHxZlURZlMYOPcRav8DNe+7Aoi7IoixksYh6ruGU6URZlURZ7dB5LeIFrphdlURZlsQef4BfM4TfcNr0oi7Ioi5HmsIoT2MCScaIsyqIsRjqJUwY/YcM4URZlURYjHMPvBov41XhRFmVRFiN8j6MGf+J/40VZlEVZTOkb/Gh2URZlURZT+hoHDTbwn72JsiiLshjpDr7Dpr2JsiiLspjSMpbNLsqiLMreApamPWWOWvFrAAAAAElFTkSuQmCC",
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAdBJREFUaAW9wb1qlgcABtCDeToUXLRU6FB/cOtSgggFWyh0EV0EvYXUoXQpBFxCQYdAxg7egeAFlBIKKXTRJYv4UyrGwYoIhQS0Q1ChDu8QBL/4vfnCc06URVmURVmURVmURVmURVmURVmURVmURVmURVmURVmURVmURVmURVmURVlMcAkLeIZt3MBzPDKbKIuyKIsJVnDcjst4ift29xQrWPd+URZlURYTLOBLPMAXmMe3+Ar/4HM73uBffGbwBOveL8qiLMpigjWsGawaHMI81nHajm08xF84jMcmi7Ioi7IYYQt/GKx510Ucwl3cNFmURVmUxT44gus4gKvYNFmURVmUxT74AZ9iC3/bXZRFWZTFjM7gisEF3LO7KIuyKIsZncNHWMNtHxZlURZlMYOPcRav8DNe+7Aoi7IoixksYh6ruGU6URZlURZ7dB5LeIFrphdlURZlsQef4BfM4TfcNr0oi7Ioi5HmsIoT2MCScaIsyqIsRjqJUwY/YcM4URZlURYjHMPvBov41XhRFmVRFiN8j6MGf+J/40VZlEVZTOkb/Gh2URZlURZT+hoHDTbwn72JsiiLshjpDr7Dpr2JsiiLspjSMpbNLsqiLMreApamPWWOWvFrAAAAAElFTkSuQmCC\">"
      ],
      "text/plain": [
       "28×28 reinterpret(reshape, Gray{Float32}, adjoint(::Matrix{Float32})) with eltype Gray{Float32}:\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " ⋮                                       ⋱  \n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view testing input\n",
    "\n",
    "colorview(Gray, X_testᵨ[:, :, index]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b22da92-8ae8-4254-841f-cd9acf722b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view testing label\n",
    "\n",
    "y_testᵨ[index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e6b15e-4eb3-4751-8eef-3d6ba00621d1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f309f725-a39d-42eb-bac1-7927f808b0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(typeof(X_train), size(X_train)) = (Matrix{Float32}, (784, 60000))\n",
      "(typeof(X_test), size(X_test)) = (Matrix{Float32}, (784, 10000))\n"
     ]
    }
   ],
   "source": [
    "# flatten input data\n",
    "\n",
    "X_train = Flux.flatten(X_trainᵨ); @show typeof(X_train), size(X_train)\n",
    "\n",
    "X_test = Flux.flatten(X_testᵨ); @show typeof(X_test), size(X_test);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de32df7c-8e8d-46ba-9d99-79baf2edda86",
   "metadata": {},
   "source": [
    "&#x1F4CD; Note: Dimensions\n",
    "\n",
    "> Hint: The digits dataset is a dataset of images. You need to convert them from a 2D array to a 1D one.\n",
    "\n",
    "Flattening allow the data to strip down from an array of three dimensions `Array{Float32, 3}, (28, 28, 60000)` to a Matrix  `Matrix{Float32}, (784, 60000)`.\n",
    "The 28x28 tensor array has been replaced to a 784 column vector where each column contains the floating point numbers associated with each image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7200787b-8163-4023-9561-1ece75d5647e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×10000 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n",
       " ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  …  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  1  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  1  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  …  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅     ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1\n",
       " 1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  1  ⋅  ⋅  1     ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encode labels\n",
    "\n",
    "y_train = onehotbatch(y_trainᵨ, 0:9)\n",
    "\n",
    "y_test = onehotbatch(y_testᵨ, 0:9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7416ac1e-3acc-4242-b576-59776d69095c",
   "metadata": {},
   "source": [
    "&#x1F4CD; Note: one-hot encoding\n",
    "\n",
    "As you can see with the above result,  the 1st label appearing in `y_testᵨ` previously viewed is a `seven` label. In the column vector, it appears at the 8th row; (marked by  a ${\\color{Green}1}$) because, the column vector starts from 0 and ends at 9 (labels). The dots represent  zeroes ${\\color{Salmon}0}$.\n",
    "\n",
    "\n",
    "As a side note, Julia' indexing is 1-based, like Matlab. Python indexing is 0-based."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb69e9b-f9e8-4549-8242-fac3817f5ce1",
   "metadata": {},
   "source": [
    "<img src=images/one-hot-label.png width='' height='' > </img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2724698f-7926-4871-969c-ce054c7b9579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(784 => 32, relu),               \u001b[90m# 25_120 parameters\u001b[39m\n",
       "  Dense(32 => 10),                      \u001b[90m# 330 parameters\u001b[39m\n",
       "  NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m25_450 parameters, 99.664 KiB."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model architecture\n",
    "\n",
    "model = Chain(\n",
    "    Dense(28 * 28, 32, relu),\n",
    "    Dense(32, 10),\n",
    "    softmax\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18709d59-2de4-4997-acdc-24c1f657f6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "𝐿 (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define loss function 𝐿\n",
    "\n",
    "𝐿(x, y) = crossentropy(model(x), y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f53b4f55-cb9a-4e21-acad-2fbe66e7751d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# track parameters called θ\n",
    "\n",
    "θ = params(model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3d04448-a8ac-4c9d-9f99-f51c55fd80c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam(0.009999999776482582, (0.9, 0.999), 1.0e-8, IdDict{Any, Any}())"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select optimizer\n",
    "\n",
    "# Let's define the learning rate η \\eta\n",
    "η = Float32(0.01)\n",
    "\n",
    "opt = ADAM(η)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7930235-7001-44f7-a66d-65a46b8a1a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1 : Training Loss = 1.723311\n",
      "Epoch = 2 : Training Loss = 1.291694\n",
      "Epoch = 3 : Training Loss = 0.9837209\n",
      "Epoch = 4 : Training Loss = 0.7877831\n",
      "Epoch = 5 : Training Loss = 0.67078894\n",
      "Epoch = 6 : Training Loss = 0.5911647\n",
      "Epoch = 7 : Training Loss = 0.5364811\n",
      "Epoch = 8 : Training Loss = 0.49772683\n",
      "Epoch = 9 : Training Loss = 0.4683123\n",
      "Epoch = 10 : Training Loss = 0.4448052\n",
      "Epoch = 11 : Training Loss = 0.42594695\n",
      "Epoch = 12 : Training Loss = 0.41077128\n",
      "Epoch = 13 : Training Loss = 0.3963644\n",
      "Epoch = 14 : Training Loss = 0.38463184\n",
      "Epoch = 15 : Training Loss = 0.3747828\n",
      "Epoch = 16 : Training Loss = 0.36559427\n",
      "Epoch = 17 : Training Loss = 0.35712177\n",
      "Epoch = 18 : Training Loss = 0.3490166\n",
      "Epoch = 19 : Training Loss = 0.34206536\n",
      "Epoch = 20 : Training Loss = 0.3351813\n",
      "Epoch = 21 : Training Loss = 0.32819563\n",
      "Epoch = 22 : Training Loss = 0.32181603\n",
      "Epoch = 23 : Training Loss = 0.3158584\n",
      "Epoch = 24 : Training Loss = 0.31042445\n",
      "Epoch = 25 : Training Loss = 0.30507693\n",
      "Epoch = 26 : Training Loss = 0.29985964\n",
      "Epoch = 27 : Training Loss = 0.29497296\n",
      "Epoch = 28 : Training Loss = 0.29014713\n",
      "Epoch = 29 : Training Loss = 0.28547773\n",
      "Epoch = 30 : Training Loss = 0.28087226\n",
      "Epoch = 31 : Training Loss = 0.2764769\n",
      "Epoch = 32 : Training Loss = 0.2723249\n",
      "Epoch = 33 : Training Loss = 0.2683595\n",
      "Epoch = 34 : Training Loss = 0.26463598\n",
      "Epoch = 35 : Training Loss = 0.26100138\n",
      "Epoch = 36 : Training Loss = 0.25755215\n",
      "Epoch = 37 : Training Loss = 0.25421908\n",
      "Epoch = 38 : Training Loss = 0.25096998\n",
      "Epoch = 39 : Training Loss = 0.24782051\n",
      "Epoch = 40 : Training Loss = 0.24479203\n",
      "Epoch = 41 : Training Loss = 0.24193044\n",
      "Epoch = 42 : Training Loss = 0.23913069\n",
      "Epoch = 43 : Training Loss = 0.23644766\n",
      "Epoch = 44 : Training Loss = 0.23385824\n",
      "Epoch = 45 : Training Loss = 0.231331\n",
      "Epoch = 46 : Training Loss = 0.22882317\n",
      "Epoch = 47 : Training Loss = 0.22635296\n",
      "Epoch = 48 : Training Loss = 0.22392641\n",
      "Epoch = 49 : Training Loss = 0.22152191\n",
      "Epoch = 50 : Training Loss = 0.21915054\n",
      "Epoch = 51 : Training Loss = 0.21682882\n",
      "Epoch = 52 : Training Loss = 0.21457969\n",
      "Epoch = 53 : Training Loss = 0.2123592\n",
      "Epoch = 54 : Training Loss = 0.21019296\n",
      "Epoch = 55 : Training Loss = 0.20807132\n",
      "Epoch = 56 : Training Loss = 0.20600273\n",
      "Epoch = 57 : Training Loss = 0.20396373\n",
      "Epoch = 58 : Training Loss = 0.20197915\n",
      "Epoch = 59 : Training Loss = 0.20003581\n",
      "Epoch = 60 : Training Loss = 0.19814238\n",
      "Epoch = 61 : Training Loss = 0.19628428\n",
      "Epoch = 62 : Training Loss = 0.19446631\n",
      "Epoch = 63 : Training Loss = 0.19267859\n",
      "Epoch = 64 : Training Loss = 0.19092757\n",
      "Epoch = 65 : Training Loss = 0.18920885\n",
      "Epoch = 66 : Training Loss = 0.18752334\n",
      "Epoch = 67 : Training Loss = 0.18586853\n",
      "Epoch = 68 : Training Loss = 0.18424556\n",
      "Epoch = 69 : Training Loss = 0.18265082\n",
      "Epoch = 70 : Training Loss = 0.18108949\n",
      "Epoch = 71 : Training Loss = 0.17955838\n",
      "Epoch = 72 : Training Loss = 0.17805557\n",
      "Epoch = 73 : Training Loss = 0.17657536\n",
      "Epoch = 74 : Training Loss = 0.17512617\n",
      "Epoch = 75 : Training Loss = 0.17370452\n",
      "Epoch = 76 : Training Loss = 0.1723109\n",
      "Epoch = 77 : Training Loss = 0.17093915\n",
      "Epoch = 78 : Training Loss = 0.169593\n",
      "Epoch = 79 : Training Loss = 0.16826995\n",
      "Epoch = 80 : Training Loss = 0.16696891\n",
      "Epoch = 81 : Training Loss = 0.16568509\n",
      "Epoch = 82 : Training Loss = 0.16441849\n",
      "Epoch = 83 : Training Loss = 0.16317005\n",
      "Epoch = 84 : Training Loss = 0.16194238\n",
      "Epoch = 85 : Training Loss = 0.16073433\n",
      "Epoch = 86 : Training Loss = 0.15954362\n",
      "Epoch = 87 : Training Loss = 0.1583693\n",
      "Epoch = 88 : Training Loss = 0.15721394\n",
      "Epoch = 89 : Training Loss = 0.15607496\n",
      "Epoch = 90 : Training Loss = 0.15495443\n",
      "Epoch = 91 : Training Loss = 0.15384834\n",
      "Epoch = 92 : Training Loss = 0.1527571\n",
      "Epoch = 93 : Training Loss = 0.15168007\n",
      "Epoch = 94 : Training Loss = 0.15061635\n",
      "Epoch = 95 : Training Loss = 0.14956398\n",
      "Epoch = 96 : Training Loss = 0.14852464\n",
      "Epoch = 97 : Training Loss = 0.147496\n",
      "Epoch = 98 : Training Loss = 0.14647622\n",
      "Epoch = 99 : Training Loss = 0.14546612\n",
      "Epoch = 100 : Training Loss = 0.14446692\n",
      "Epoch = 101 : Training Loss = 0.14347985\n",
      "Epoch = 102 : Training Loss = 0.14250535\n",
      "Epoch = 103 : Training Loss = 0.14154391\n",
      "Epoch = 104 : Training Loss = 0.1405931\n",
      "Epoch = 105 : Training Loss = 0.13965504\n",
      "Epoch = 106 : Training Loss = 0.13872986\n",
      "Epoch = 107 : Training Loss = 0.13781522\n",
      "Epoch = 108 : Training Loss = 0.13691369\n",
      "Epoch = 109 : Training Loss = 0.13602272\n",
      "Epoch = 110 : Training Loss = 0.13514383\n",
      "Epoch = 111 : Training Loss = 0.13427393\n",
      "Epoch = 112 : Training Loss = 0.1334138\n",
      "Epoch = 113 : Training Loss = 0.13256188\n",
      "Epoch = 114 : Training Loss = 0.13172019\n",
      "Epoch = 115 : Training Loss = 0.13088849\n",
      "Epoch = 116 : Training Loss = 0.13006712\n",
      "Epoch = 117 : Training Loss = 0.12925442\n",
      "Epoch = 118 : Training Loss = 0.12844768\n",
      "Epoch = 119 : Training Loss = 0.12764822\n",
      "Epoch = 120 : Training Loss = 0.1268573\n",
      "Epoch = 121 : Training Loss = 0.12607546\n",
      "Epoch = 122 : Training Loss = 0.12530152\n",
      "Epoch = 123 : Training Loss = 0.12453215\n",
      "Epoch = 124 : Training Loss = 0.123769\n",
      "Epoch = 125 : Training Loss = 0.12301315\n",
      "Epoch = 126 : Training Loss = 0.12226585\n",
      "Epoch = 127 : Training Loss = 0.12152468\n",
      "Epoch = 128 : Training Loss = 0.12078892\n",
      "Epoch = 129 : Training Loss = 0.120062225\n",
      "Epoch = 130 : Training Loss = 0.11933751\n",
      "Epoch = 131 : Training Loss = 0.118618816\n",
      "Epoch = 132 : Training Loss = 0.11790705\n",
      "Epoch = 133 : Training Loss = 0.11720242\n",
      "Epoch = 134 : Training Loss = 0.11650317\n",
      "Epoch = 135 : Training Loss = 0.11581244\n",
      "Epoch = 136 : Training Loss = 0.115130715\n",
      "Epoch = 137 : Training Loss = 0.11445442\n",
      "Epoch = 138 : Training Loss = 0.11378599\n",
      "Epoch = 139 : Training Loss = 0.113122724\n",
      "Epoch = 140 : Training Loss = 0.11246405\n",
      "Epoch = 141 : Training Loss = 0.111810744\n",
      "Epoch = 142 : Training Loss = 0.11116449\n",
      "Epoch = 143 : Training Loss = 0.11052591\n",
      "Epoch = 144 : Training Loss = 0.10989414\n",
      "Epoch = 145 : Training Loss = 0.10926924\n",
      "Epoch = 146 : Training Loss = 0.108652756\n",
      "Epoch = 147 : Training Loss = 0.10803755\n",
      "Epoch = 148 : Training Loss = 0.10742733\n",
      "Epoch = 149 : Training Loss = 0.106826365\n",
      "Epoch = 150 : Training Loss = 0.10622835\n",
      "Epoch = 151 : Training Loss = 0.10563563\n",
      "Epoch = 152 : Training Loss = 0.105050415\n",
      "Epoch = 153 : Training Loss = 0.104470916\n",
      "Epoch = 154 : Training Loss = 0.10389825\n",
      "Epoch = 155 : Training Loss = 0.10333225\n",
      "Epoch = 156 : Training Loss = 0.102767974\n",
      "Epoch = 157 : Training Loss = 0.102209866\n",
      "Epoch = 158 : Training Loss = 0.10166238\n",
      "Epoch = 159 : Training Loss = 0.101125285\n",
      "Epoch = 160 : Training Loss = 0.10059581\n",
      "Epoch = 161 : Training Loss = 0.10005553\n",
      "Epoch = 162 : Training Loss = 0.09952401\n",
      "Epoch = 163 : Training Loss = 0.09900623\n",
      "Epoch = 164 : Training Loss = 0.09848218\n",
      "Epoch = 165 : Training Loss = 0.09796304\n",
      "Epoch = 166 : Training Loss = 0.09745337\n",
      "Epoch = 167 : Training Loss = 0.096948944\n",
      "Epoch = 168 : Training Loss = 0.09645106\n",
      "Epoch = 169 : Training Loss = 0.09595694\n",
      "Epoch = 170 : Training Loss = 0.095467046\n",
      "Epoch = 171 : Training Loss = 0.094985984\n",
      "Epoch = 172 : Training Loss = 0.09451074\n",
      "Epoch = 173 : Training Loss = 0.094044045\n",
      "Epoch = 174 : Training Loss = 0.093566485\n",
      "Epoch = 175 : Training Loss = 0.09309426\n",
      "Epoch = 176 : Training Loss = 0.09263516\n",
      "Epoch = 177 : Training Loss = 0.09218297\n",
      "Epoch = 178 : Training Loss = 0.09172869\n",
      "Epoch = 179 : Training Loss = 0.091273256\n",
      "Epoch = 180 : Training Loss = 0.09082537\n",
      "Epoch = 181 : Training Loss = 0.09038565\n",
      "Epoch = 182 : Training Loss = 0.08995462\n",
      "Epoch = 183 : Training Loss = 0.0895305\n",
      "Epoch = 184 : Training Loss = 0.08910034\n",
      "Epoch = 185 : Training Loss = 0.088667415\n",
      "Epoch = 186 : Training Loss = 0.08823957\n",
      "Epoch = 187 : Training Loss = 0.08782717\n",
      "Epoch = 188 : Training Loss = 0.087429926\n",
      "Epoch = 189 : Training Loss = 0.087013945\n",
      "Epoch = 190 : Training Loss = 0.086600654\n",
      "Epoch = 191 : Training Loss = 0.086186364\n",
      "Epoch = 192 : Training Loss = 0.08579515\n",
      "Epoch = 193 : Training Loss = 0.085412435\n",
      "Epoch = 194 : Training Loss = 0.08501109\n",
      "Epoch = 195 : Training Loss = 0.084611185\n",
      "Epoch = 196 : Training Loss = 0.08423136\n",
      "Epoch = 197 : Training Loss = 0.08385346\n",
      "Epoch = 198 : Training Loss = 0.08347277\n",
      "Epoch = 199 : Training Loss = 0.083081864\n",
      "Epoch = 200 : Training Loss = 0.082713306\n",
      "Epoch = 201 : Training Loss = 0.08235493\n",
      "Epoch = 202 : Training Loss = 0.08198086\n",
      "Epoch = 203 : Training Loss = 0.08160227\n",
      "Epoch = 204 : Training Loss = 0.08123979\n",
      "Epoch = 205 : Training Loss = 0.080887996\n",
      "Epoch = 206 : Training Loss = 0.08053582\n",
      "Epoch = 207 : Training Loss = 0.08017259\n",
      "Epoch = 208 : Training Loss = 0.07981316\n",
      "Epoch = 209 : Training Loss = 0.07946206\n",
      "Epoch = 210 : Training Loss = 0.07912137\n",
      "Epoch = 211 : Training Loss = 0.07879092\n",
      "Epoch = 212 : Training Loss = 0.078439824\n",
      "Epoch = 213 : Training Loss = 0.07808794\n",
      "Epoch = 214 : Training Loss = 0.07774751\n",
      "Epoch = 215 : Training Loss = 0.077419676\n",
      "Epoch = 216 : Training Loss = 0.07710295\n",
      "Epoch = 217 : Training Loss = 0.07676876\n",
      "Epoch = 218 : Training Loss = 0.076434776\n",
      "Epoch = 219 : Training Loss = 0.07609679\n",
      "Epoch = 220 : Training Loss = 0.07577544\n",
      "Epoch = 221 : Training Loss = 0.07546156\n",
      "Epoch = 222 : Training Loss = 0.07514241\n",
      "Epoch = 223 : Training Loss = 0.07482149\n",
      "Epoch = 224 : Training Loss = 0.07450376\n",
      "Epoch = 225 : Training Loss = 0.074192256\n",
      "Epoch = 226 : Training Loss = 0.0738815\n",
      "Epoch = 227 : Training Loss = 0.07357399\n",
      "Epoch = 228 : Training Loss = 0.07327438\n",
      "Epoch = 229 : Training Loss = 0.07297783\n",
      "Epoch = 230 : Training Loss = 0.072686195\n",
      "Epoch = 231 : Training Loss = 0.0723703\n",
      "Epoch = 232 : Training Loss = 0.07205499\n",
      "Epoch = 233 : Training Loss = 0.071756855\n",
      "Epoch = 234 : Training Loss = 0.07146818\n",
      "Epoch = 235 : Training Loss = 0.071184285\n",
      "Epoch = 236 : Training Loss = 0.070893735\n",
      "Epoch = 237 : Training Loss = 0.070596576\n",
      "Epoch = 238 : Training Loss = 0.070301525\n",
      "Epoch = 239 : Training Loss = 0.070007965\n",
      "Epoch = 240 : Training Loss = 0.06972038\n",
      "Epoch = 241 : Training Loss = 0.06944388\n",
      "Epoch = 242 : Training Loss = 0.06918318\n",
      "Epoch = 243 : Training Loss = 0.068902746\n",
      "Epoch = 244 : Training Loss = 0.06861977\n",
      "Epoch = 245 : Training Loss = 0.068323754\n",
      "Epoch = 246 : Training Loss = 0.06804154\n",
      "Epoch = 247 : Training Loss = 0.067773506\n",
      "Epoch = 248 : Training Loss = 0.06751712\n",
      "Epoch = 249 : Training Loss = 0.06725056\n",
      "Epoch = 250 : Training Loss = 0.06696924\n",
      "Epoch = 251 : Training Loss = 0.06669248\n",
      "Epoch = 252 : Training Loss = 0.066420235\n",
      "Epoch = 253 : Training Loss = 0.06615526\n",
      "Epoch = 254 : Training Loss = 0.06589274\n",
      "Epoch = 255 : Training Loss = 0.06563547\n",
      "Epoch = 256 : Training Loss = 0.06538194\n",
      "Epoch = 257 : Training Loss = 0.065132014\n",
      "Epoch = 258 : Training Loss = 0.064868756\n",
      "Epoch = 259 : Training Loss = 0.06460765\n",
      "Epoch = 260 : Training Loss = 0.06434696\n",
      "Epoch = 261 : Training Loss = 0.064085834\n",
      "Epoch = 262 : Training Loss = 0.063827924\n",
      "Epoch = 263 : Training Loss = 0.063575245\n",
      "Epoch = 264 : Training Loss = 0.06333361\n",
      "Epoch = 265 : Training Loss = 0.06309938\n",
      "Epoch = 266 : Training Loss = 0.06287052\n",
      "Epoch = 267 : Training Loss = 0.06261672\n",
      "Epoch = 268 : Training Loss = 0.062360674\n",
      "Epoch = 269 : Training Loss = 0.062090784\n",
      "Epoch = 270 : Training Loss = 0.06183383\n",
      "Epoch = 271 : Training Loss = 0.061586283\n",
      "Epoch = 272 : Training Loss = 0.061347596\n",
      "Epoch = 273 : Training Loss = 0.06112714\n",
      "Epoch = 274 : Training Loss = 0.060901705\n",
      "Epoch = 275 : Training Loss = 0.060679127\n",
      "Epoch = 276 : Training Loss = 0.060424097\n",
      "Epoch = 277 : Training Loss = 0.0601736\n",
      "Epoch = 278 : Training Loss = 0.05992234\n",
      "Epoch = 279 : Training Loss = 0.059676602\n",
      "Epoch = 280 : Training Loss = 0.059436493\n",
      "Epoch = 281 : Training Loss = 0.059204135\n",
      "Epoch = 282 : Training Loss = 0.058975156\n",
      "Epoch = 283 : Training Loss = 0.058747232\n",
      "Epoch = 284 : Training Loss = 0.058518\n",
      "Epoch = 285 : Training Loss = 0.058293432\n",
      "Epoch = 286 : Training Loss = 0.058090128\n",
      "Epoch = 287 : Training Loss = 0.057869937\n",
      "Epoch = 288 : Training Loss = 0.057645604\n",
      "Epoch = 289 : Training Loss = 0.057397317\n",
      "Epoch = 290 : Training Loss = 0.057161354\n",
      "Epoch = 291 : Training Loss = 0.05693636\n",
      "Epoch = 292 : Training Loss = 0.056714255\n",
      "Epoch = 293 : Training Loss = 0.056489576\n",
      "Epoch = 294 : Training Loss = 0.056269646\n",
      "Epoch = 295 : Training Loss = 0.056054648\n",
      "Epoch = 296 : Training Loss = 0.05583345\n",
      "Epoch = 297 : Training Loss = 0.055626366\n",
      "Epoch = 298 : Training Loss = 0.05541962\n",
      "Epoch = 299 : Training Loss = 0.05522338\n",
      "Epoch = 300 : Training Loss = 0.055022974\n",
      "Epoch = 301 : Training Loss = 0.054826304\n",
      "Epoch = 302 : Training Loss = 0.05460933\n",
      "Epoch = 303 : Training Loss = 0.05437763\n",
      "Epoch = 304 : Training Loss = 0.05414248\n",
      "Epoch = 305 : Training Loss = 0.053910486\n",
      "Epoch = 306 : Training Loss = 0.05368325\n",
      "Epoch = 307 : Training Loss = 0.053463273\n",
      "Epoch = 308 : Training Loss = 0.053258076\n",
      "Epoch = 309 : Training Loss = 0.053065483\n",
      "Epoch = 310 : Training Loss = 0.052891884\n",
      "Epoch = 311 : Training Loss = 0.05272071\n",
      "Epoch = 312 : Training Loss = 0.052560452\n",
      "Epoch = 313 : Training Loss = 0.05238295\n",
      "Epoch = 314 : Training Loss = 0.052196223\n",
      "Epoch = 315 : Training Loss = 0.05198357\n",
      "Epoch = 316 : Training Loss = 0.051748585\n",
      "Epoch = 317 : Training Loss = 0.051494647\n",
      "Epoch = 318 : Training Loss = 0.051244263\n",
      "Epoch = 319 : Training Loss = 0.051013842\n",
      "Epoch = 320 : Training Loss = 0.050812405\n",
      "Epoch = 321 : Training Loss = 0.050627604\n",
      "Epoch = 322 : Training Loss = 0.050449837\n",
      "Epoch = 323 : Training Loss = 0.050264303\n",
      "Epoch = 324 : Training Loss = 0.050070696\n",
      "Epoch = 325 : Training Loss = 0.049862873\n",
      "Epoch = 326 : Training Loss = 0.04965047\n",
      "Epoch = 327 : Training Loss = 0.04944025\n",
      "Epoch = 328 : Training Loss = 0.049243968\n",
      "Epoch = 329 : Training Loss = 0.04905908\n",
      "Epoch = 330 : Training Loss = 0.048879385\n",
      "Epoch = 331 : Training Loss = 0.048729863\n",
      "Epoch = 332 : Training Loss = 0.048531532\n",
      "Epoch = 333 : Training Loss = 0.048333284\n",
      "Epoch = 334 : Training Loss = 0.048150886\n",
      "Epoch = 335 : Training Loss = 0.047993887\n",
      "Epoch = 336 : Training Loss = 0.04786034\n",
      "Epoch = 337 : Training Loss = 0.047721297\n",
      "Epoch = 338 : Training Loss = 0.04755957\n",
      "Epoch = 339 : Training Loss = 0.047372658\n",
      "Epoch = 340 : Training Loss = 0.04717583\n",
      "Epoch = 341 : Training Loss = 0.046986535\n",
      "Epoch = 342 : Training Loss = 0.046807446\n",
      "Epoch = 343 : Training Loss = 0.046630304\n",
      "Epoch = 344 : Training Loss = 0.04636067\n",
      "Epoch = 345 : Training Loss = 0.046108063\n",
      "Epoch = 346 : Training Loss = 0.045895603\n",
      "Epoch = 347 : Training Loss = 0.045740303\n",
      "Epoch = 348 : Training Loss = 0.04560686\n",
      "Epoch = 349 : Training Loss = 0.045431003\n",
      "Epoch = 350 : Training Loss = 0.045256946\n",
      "Epoch = 351 : Training Loss = 0.045083992\n",
      "Epoch = 352 : Training Loss = 0.044951305\n",
      "Epoch = 353 : Training Loss = 0.044849772\n",
      "Epoch = 354 : Training Loss = 0.044683352\n",
      "Epoch = 355 : Training Loss = 0.04447758\n",
      "Epoch = 356 : Training Loss = 0.044231094\n",
      "Epoch = 357 : Training Loss = 0.044020724\n",
      "Epoch = 358 : Training Loss = 0.04387021\n",
      "Epoch = 359 : Training Loss = 0.043718636\n",
      "Epoch = 360 : Training Loss = 0.043532114\n",
      "Epoch = 361 : Training Loss = 0.0432987\n",
      "Epoch = 362 : Training Loss = 0.04310305\n",
      "Epoch = 363 : Training Loss = 0.042955905\n",
      "Epoch = 364 : Training Loss = 0.04282801\n",
      "Epoch = 365 : Training Loss = 0.042703953\n",
      "Epoch = 366 : Training Loss = 0.042490993\n",
      "Epoch = 367 : Training Loss = 0.04228236\n",
      "Epoch = 368 : Training Loss = 0.042105228\n",
      "Epoch = 369 : Training Loss = 0.041959066\n",
      "Epoch = 370 : Training Loss = 0.041837692\n",
      "Epoch = 371 : Training Loss = 0.04168821\n",
      "Epoch = 372 : Training Loss = 0.041520216\n",
      "Epoch = 373 : Training Loss = 0.04134952\n",
      "Epoch = 374 : Training Loss = 0.041204274\n",
      "Epoch = 375 : Training Loss = 0.04109907\n",
      "Epoch = 376 : Training Loss = 0.04106373\n",
      "Epoch = 377 : Training Loss = 0.041107275\n",
      "Epoch = 378 : Training Loss = 0.041168276\n",
      "Epoch = 379 : Training Loss = 0.041215967\n",
      "Epoch = 380 : Training Loss = 0.041225158\n",
      "Epoch = 381 : Training Loss = 0.041033175\n",
      "Epoch = 382 : Training Loss = 0.040655542\n",
      "Epoch = 383 : Training Loss = 0.04010604\n",
      "Epoch = 384 : Training Loss = 0.039664436\n",
      "Epoch = 385 : Training Loss = 0.039515227\n",
      "Epoch = 386 : Training Loss = 0.03957396\n",
      "Epoch = 387 : Training Loss = 0.03963231\n",
      "Epoch = 388 : Training Loss = 0.03951397\n",
      "Epoch = 389 : Training Loss = 0.039180707\n",
      "Epoch = 390 : Training Loss = 0.038806755\n",
      "Epoch = 391 : Training Loss = 0.038548812\n",
      "Epoch = 392 : Training Loss = 0.038465954\n",
      "Epoch = 393 : Training Loss = 0.038480237\n",
      "Epoch = 394 : Training Loss = 0.03841325\n",
      "Epoch = 395 : Training Loss = 0.0382436\n",
      "Epoch = 396 : Training Loss = 0.037989054\n",
      "Epoch = 397 : Training Loss = 0.037722245\n",
      "Epoch = 398 : Training Loss = 0.037528884\n",
      "Epoch = 399 : Training Loss = 0.037419915\n",
      "Epoch = 400 : Training Loss = 0.037349463\n",
      "Epoch = 401 : Training Loss = 0.03726553\n",
      "Epoch = 402 : Training Loss = 0.03713238\n",
      "Epoch = 403 : Training Loss = 0.036950644\n",
      "Epoch = 404 : Training Loss = 0.03675255\n",
      "Epoch = 405 : Training Loss = 0.036577906\n",
      "Epoch = 406 : Training Loss = 0.036424253\n",
      "Epoch = 407 : Training Loss = 0.03631434\n",
      "Epoch = 408 : Training Loss = 0.03621259\n",
      "Epoch = 409 : Training Loss = 0.036125943\n",
      "Epoch = 410 : Training Loss = 0.03602216\n",
      "Epoch = 411 : Training Loss = 0.035911392\n",
      "Epoch = 412 : Training Loss = 0.035771776\n",
      "Epoch = 413 : Training Loss = 0.03560862\n",
      "Epoch = 414 : Training Loss = 0.035430104\n",
      "Epoch = 415 : Training Loss = 0.03526085\n",
      "Epoch = 416 : Training Loss = 0.035107367\n",
      "Epoch = 417 : Training Loss = 0.034977466\n",
      "Epoch = 418 : Training Loss = 0.034846265\n",
      "Epoch = 419 : Training Loss = 0.03472874\n",
      "Epoch = 420 : Training Loss = 0.03461363\n",
      "Epoch = 421 : Training Loss = 0.034503214\n",
      "Epoch = 422 : Training Loss = 0.034392506\n",
      "Epoch = 423 : Training Loss = 0.03426967\n",
      "Epoch = 424 : Training Loss = 0.03413309\n",
      "Epoch = 425 : Training Loss = 0.03399564\n",
      "Epoch = 426 : Training Loss = 0.033866316\n",
      "Epoch = 427 : Training Loss = 0.033750597\n",
      "Epoch = 428 : Training Loss = 0.033662423\n",
      "Epoch = 429 : Training Loss = 0.033591162\n",
      "Epoch = 430 : Training Loss = 0.033538576\n",
      "Epoch = 431 : Training Loss = 0.033434108\n",
      "Epoch = 432 : Training Loss = 0.03329637\n",
      "Epoch = 433 : Training Loss = 0.03316149\n",
      "Epoch = 434 : Training Loss = 0.033040337\n",
      "Epoch = 435 : Training Loss = 0.032932602\n",
      "Epoch = 436 : Training Loss = 0.032804243\n",
      "Epoch = 437 : Training Loss = 0.03265713\n",
      "Epoch = 438 : Training Loss = 0.032494333\n",
      "Epoch = 439 : Training Loss = 0.032314487\n",
      "Epoch = 440 : Training Loss = 0.032149505\n",
      "Epoch = 441 : Training Loss = 0.03200448\n",
      "Epoch = 442 : Training Loss = 0.03189922\n",
      "Epoch = 443 : Training Loss = 0.031830348\n",
      "Epoch = 444 : Training Loss = 0.03176445\n",
      "Epoch = 445 : Training Loss = 0.03170926\n",
      "Epoch = 446 : Training Loss = 0.031646673\n",
      "Epoch = 447 : Training Loss = 0.03160544\n",
      "Epoch = 448 : Training Loss = 0.031583242\n",
      "Epoch = 449 : Training Loss = 0.03151402\n",
      "Epoch = 450 : Training Loss = 0.031429235\n",
      "Epoch = 451 : Training Loss = 0.031261515\n",
      "Epoch = 452 : Training Loss = 0.031050505\n",
      "Epoch = 453 : Training Loss = 0.03081338\n",
      "Epoch = 454 : Training Loss = 0.030574638\n",
      "Epoch = 455 : Training Loss = 0.03039739\n",
      "Epoch = 456 : Training Loss = 0.030290358\n",
      "Epoch = 457 : Training Loss = 0.030237472\n",
      "Epoch = 458 : Training Loss = 0.030226067\n",
      "Epoch = 459 : Training Loss = 0.030179387\n",
      "Epoch = 460 : Training Loss = 0.030086238\n",
      "Epoch = 461 : Training Loss = 0.029978024\n",
      "Epoch = 462 : Training Loss = 0.029837793\n",
      "Epoch = 463 : Training Loss = 0.02968638\n",
      "Epoch = 464 : Training Loss = 0.029523551\n",
      "Epoch = 465 : Training Loss = 0.029377926\n",
      "Epoch = 466 : Training Loss = 0.029260047\n",
      "Epoch = 467 : Training Loss = 0.029164614\n",
      "Epoch = 468 : Training Loss = 0.029071765\n",
      "Epoch = 469 : Training Loss = 0.028959453\n",
      "Epoch = 470 : Training Loss = 0.02887266\n",
      "Epoch = 471 : Training Loss = 0.02879273\n",
      "Epoch = 472 : Training Loss = 0.02870123\n",
      "Epoch = 473 : Training Loss = 0.028607711\n",
      "Epoch = 474 : Training Loss = 0.02849037\n",
      "Epoch = 475 : Training Loss = 0.028359046\n",
      "Epoch = 476 : Training Loss = 0.028225444\n",
      "Epoch = 477 : Training Loss = 0.028076893\n",
      "Epoch = 478 : Training Loss = 0.027934527\n",
      "Epoch = 479 : Training Loss = 0.027806308\n",
      "Epoch = 480 : Training Loss = 0.027705684\n",
      "Epoch = 481 : Training Loss = 0.02762643\n",
      "Epoch = 482 : Training Loss = 0.027528442\n",
      "Epoch = 483 : Training Loss = 0.0274424\n",
      "Epoch = 484 : Training Loss = 0.027356926\n",
      "Epoch = 485 : Training Loss = 0.02725116\n",
      "Epoch = 486 : Training Loss = 0.027175784\n",
      "Epoch = 487 : Training Loss = 0.027100809\n",
      "Epoch = 488 : Training Loss = 0.027031776\n",
      "Epoch = 489 : Training Loss = 0.027008653\n",
      "Epoch = 490 : Training Loss = 0.027028667\n",
      "Epoch = 491 : Training Loss = 0.02704857\n",
      "Epoch = 492 : Training Loss = 0.027011322\n",
      "Epoch = 493 : Training Loss = 0.026981404\n",
      "Epoch = 494 : Training Loss = 0.026817264\n",
      "Epoch = 495 : Training Loss = 0.02665243\n",
      "Epoch = 496 : Training Loss = 0.0264501\n",
      "Epoch = 497 : Training Loss = 0.026197493\n",
      "Epoch = 498 : Training Loss = 0.02603203\n",
      "Epoch = 499 : Training Loss = 0.025946407\n",
      "Epoch = 500 : Training Loss = 0.025871966\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "\n",
    "# Let's keep track of the loss history\n",
    "loss_history = []\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "for epoch in 1:epochs\n",
    "    # train model\n",
    "    train!(𝐿, θ, [(X_train, y_train)], opt)\n",
    "    # print report\n",
    "    train_loss = 𝐿(X_train, y_train)\n",
    "    push!(loss_history, train_loss)\n",
    "    println(\"Epoch = $epoch : Training Loss = $train_loss\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e15e82bb-49cf-4a35-a1b4-88e4f65713ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9624"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions\n",
    "\n",
    "# Let's call the predicted variable ŷ (Latex: \\hat{y} or y\\hat)\n",
    "\n",
    "ŷᵨ = model(X_test)\n",
    "\n",
    "ŷ = onecold(ŷᵨ) .- 1\n",
    "\n",
    "y = y_testᵨ\n",
    "\n",
    "mean(ŷ .== y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "850c838b-cd31-4eb5-9524-abb64fe26160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000×4 Matrix{Int64}:\n",
       "     1  7  7  1\n",
       "     2  2  2  1\n",
       "     3  1  1  1\n",
       "     4  0  0  1\n",
       "     5  4  4  1\n",
       "     6  1  1  1\n",
       "     7  4  4  1\n",
       "     8  9  9  1\n",
       "     9  4  5  0\n",
       "    10  9  9  1\n",
       "    11  0  0  1\n",
       "    12  6  6  1\n",
       "    13  9  9  1\n",
       "     ⋮        \n",
       "  9989  5  5  1\n",
       "  9990  6  6  1\n",
       "  9991  7  7  1\n",
       "  9992  8  8  1\n",
       "  9993  9  9  1\n",
       "  9994  8  0  0\n",
       "  9995  1  1  1\n",
       "  9996  2  2  1\n",
       "  9997  3  3  1\n",
       "  9998  4  4  1\n",
       "  9999  5  5  1\n",
       " 10000  6  6  1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display results\n",
    "\n",
    "check = [ŷ[i] == y[i] for i in 1:length(y)]\n",
    "\n",
    "index = collect(1:length(y))\n",
    "\n",
    "check_display = [index ŷ y check]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa1b24c6-5669-49b7-935f-f10c727f52ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(y[misclass_index], ŷ[misclass_index]) = (5, 4)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAhhJREFUaAW9wb+rlQUABuBHe8tAGwJtqtuQuDQYiRBBYLQISRRI5NBYgw2piwZiCkpD/QFBQ8MlB8cgiEtBQ2DRcAkUQxquchv6sTiYECkN33CGvuM5pw7v80RZlEVZlEVZlEVZlEVZlEVZlEVZlEVZlMUSPIoVEzdwHFdwHT+aiLIoi7L4H17GKziA3Sau40lsM3jARJRFWZTFgp7CO3gbD2OLf9tjuiiLsiiLBT2Od033E66aLsqiLMpihp04hm/xJf7CLdzGdqzhCr7HOu7gtumiLMqiLO5jO9awF68ZXMaz2MAKNnHP/KIsyqIspngIF7EXH+ArExsGNy0uyqIsymLEDryHQ/gDH+JPyxFlURZlMeJVnMJNvIBblifKoizKYsTzBuvYtFxRFmVRFiMOGxzE+/gc65YjyqIsymLELtzDNpzBaXyM77CCn3HV4Glcxqb5RFmURVmM+AgnTGzFURw17nd8gzfMFmVRFmUx4hQu4TMET2Cr6XbhME7jvPuLsiiLshhxFz9gj8FLeBBnsd+4LdhntiiLsiiLOXxt8Az24298ik9wHEfML8qiLMpiAWu4gOAt7MYBE7+YLcqiLMpiAddwCa8bvGhwF1/gpNmiLMqiLBZwB8fwCPbhMWxgFWfNJ8qiLMpiQb/iEN7EcziH38wvyqIsyuI/WsWqxUVZlEVZlEVZlEVZlEVZlEVZlEVZlEXZP7tATad0MYkTAAAAAElFTkSuQmCC",
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAhhJREFUaAW9wb+rlQUABuBHe8tAGwJtqtuQuDQYiRBBYLQISRRI5NBYgw2piwZiCkpD/QFBQ8MlB8cgiEtBQ2DRcAkUQxquchv6sTiYECkN33CGvuM5pw7v80RZlEVZlEVZlEVZlEVZlEVZlEVZlEVZlMUSPIoVEzdwHFdwHT+aiLIoi7L4H17GKziA3Sau40lsM3jARJRFWZTFgp7CO3gbD2OLf9tjuiiLsiiLBT2Od033E66aLsqiLMpihp04hm/xJf7CLdzGdqzhCr7HOu7gtumiLMqiLO5jO9awF68ZXMaz2MAKNnHP/KIsyqIspngIF7EXH+ArExsGNy0uyqIsymLEDryHQ/gDH+JPyxFlURZlMeJVnMJNvIBblifKoizKYsTzBuvYtFxRFmVRFiMOGxzE+/gc65YjyqIsymLELtzDNpzBaXyM77CCn3HV4Glcxqb5RFmURVmM+AgnTGzFURw17nd8gzfMFmVRFmUx4hQu4TMET2Cr6XbhME7jvPuLsiiLshhxFz9gj8FLeBBnsd+4LdhntiiLsiiLOXxt8Az24298ik9wHEfML8qiLMpiAWu4gOAt7MYBE7+YLcqiLMpiAddwCa8bvGhwF1/gpNmiLMqiLBZwB8fwCPbhMWxgFWfNJ8qiLMpiQb/iEN7EcziH38wvyqIsyuI/WsWqxUVZlEVZlEVZlEVZlEVZlEVZlEVZlEXZP7tATad0MYkTAAAAAElFTkSuQmCC\">"
      ],
      "text/plain": [
       "28×28 reinterpret(reshape, Gray{Float32}, adjoint(::Matrix{Float32})) with eltype Gray{Float32}:\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " ⋮                                       ⋱  \n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view misclassifications\n",
    "\n",
    "misclass_index = 9\n",
    "img = X_testᵨ[:, :, misclass_index]\n",
    "\n",
    "@show y[misclass_index], ŷ[misclass_index];\n",
    "\n",
    "colorview(Gray, img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d552a462-682f-4e26-a0e1-3e38426ec732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize plot\n",
    "\n",
    "gr(size = (600, 600))\n",
    "\n",
    "# plot learning curve\n",
    "\n",
    "p_l_curve = plot(1:epochs, loss_history,\n",
    "    xlabel = \"Epochs\",\n",
    "    ylabel = \"Loss\",\n",
    "    title = \"Learning Curve\",\n",
    "    legend = false,\n",
    "    color = :blue,\n",
    "    linewidth = 2\n",
    ")\n",
    "# save plot\n",
    "savefig(p_l_curve, \"images/ann_learning_curve.svg\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582bb861-19e8-4451-b231-d650b0a6fbec",
   "metadata": {},
   "source": [
    "<img src=images/ann_learning_curve.svg width='' height='' > </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb76e99-7526-40f6-9c87-791665818371",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [ ] [Is there a way to print loss from Flux.train?](https://stackoverflow.com/questions/73737260/is-there-a-way-to-print-loss-from-flux-train)\n",
    "- [ ] [The Future of Machine Learning and why it looks a lot like Julia 🤖](https://towardsdatascience.com/the-future-of-machine-learning-and-why-it-looks-a-lot-like-julia-a0e26b51f6a6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b50877-16c3-41a1-bfb2-f0d4d032856e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
