{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f75bc70e-8026-4402-b67d-3878e6d95fd6",
   "metadata": {},
   "source": [
    "### Training a ground-up MLP in native Julia (yes, on MNIST)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859b3fe9-40b7-401e-b540-cb07e6a433d2",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f7/MnistExamplesModified.png\" width='50%' height='50%' ></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c7daa6-e99d-46af-a761-7b6aeb2134a0",
   "metadata": {},
   "source": [
    "**Part 1** will focus on building from the ground-up a neural network capable of a simple forward pass. We’ll train it with the backpropagation algorithm in **Part 2**, and evaluate it in **Part 3**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2458197a-51d1-4436-a99d-3393abdabdf1",
   "metadata": {},
   "source": [
    "#### Part 1 — A forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9204910e-95fd-49e7-b546-7aeba64f493c",
   "metadata": {},
   "source": [
    "$$y = g( W_3 f ( W_2 f( W_1 X+ b_1 ) + b_2 ) + b_3)$$\n",
    "\n",
    "In which $W_n$ are our weight matrices, and $b_n$ are our bias vectors. The activation functions $f, g$ we use in this model are a **ReLU** and a **Softmax** respectively. $y$ (our prediction) approximates $Y$ (the true value) as a function of observations (pixel values) $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e18df5a-028f-4677-a473-4ec344394ea1",
   "metadata": {},
   "source": [
    "#### Building blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63fc5cc-629f-40a0-9a87-114eaf5b3580",
   "metadata": {},
   "source": [
    "Julia is not really meant to be used as an object-oriented language like Python. While you can define a _struct_, this is a `\"composite data type”` rather than a **Python** _object_, and does not have _methods_ defined in its definition. There’s no sense of calling `neural_net.forward()` in which we can access `neural_net.attribute_1`, for example.\n",
    "\n",
    "Instead, in **Julia** we rely on `“dynamic dispatch”`, in which _functions_ are defined uniquely for varied input data types. So `forward(x:Array{Float64,2})` , can call completely different code to `forward(x:Int)`. The concept is similar to `“overloading”` functions in eg. **Java**, but as [this](https://stackoverflow.com/questions/50583861/overloading-vs-overriding-in-julia) interesting **StackOverflow** post points out, overloading corresponds to deciding the function to be called at compile time, while in **Julia** we decide at run time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaff10a-8103-4192-8ecc-37e722d80085",
   "metadata": {},
   "source": [
    "So, our key MLP building block is the dense layer:\n",
    "```julia\n",
    "mutable struct Dense\n",
    "    # Fully connected layer. By default has weights, biases, and an activation function.\n",
    "    weight\n",
    "    bias\n",
    "    act_fn\n",
    "    Dense(dim_in, dim_out, act_fn)=new(kaiming(Float64, dim_out, dim_in), \n",
    "          zeros(Float64, dim_out, 1), act_fn)\n",
    "end;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e502057b-29c5-4edc-9dcd-f83a080c52b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"components_dense.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee4752b-53b1-4f79-84d4-13bd0e526ca8",
   "metadata": {},
   "source": [
    "We implement this as a _struct_ , with a _weight_, _bias_ , and an _anact_fn_. Note that importantly it’s _mutable_, its attributes (the **MLP parameters**) are allowed to change. For quality of life we don’t want to instantiate this layer using the syntax: `layer_1 = Dense(W_1, b_1, ReLU())`, as this would require defining the matrix and vector elsewhere, and that’s messy. Instead, we overload `Dense()` to take dimensionality integers as input, and generate in-place a new **Kaiming init’d matrix** for the weights, and zeros for the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d8af68d-b8d6-43a8-8bc1-a759f3fba2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"components_kaiming.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6347965f-b290-4f49-9e99-e92bafdc5a7f",
   "metadata": {},
   "source": [
    "For an instance _layer_ of _Dense_, we can now call _layer.weight_ , _layer.bias_ , _layer.act_fn_ as we require."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf3fe72-5751-4a88-bddc-9d4ba4f59e45",
   "metadata": {},
   "source": [
    "#### Non-linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066e64d8-9fc6-489a-a3e5-62b997c843dc",
   "metadata": {},
   "source": [
    "For the activation functions we also utilise a _struct_, with dynamically dispatched _forward()_ and _gradient()_ methods. Every binary operator in **Julia** has a vectorised version denoted by the _._ prefix. This is similar to behaviour in MATLAB, and one of my personal favourite features of that language. These are very helpful for implementing _pointwise activation functions_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e772735f-cbd2-45d6-8ae5-ff73ed190553",
   "metadata": {},
   "source": [
    "```julia\n",
    "struct ReLU\n",
    "end;\n",
    "forward(z::Array{Float64, 2}, act_fn::ReLU)::Array{Float64,2} = z.*(z.>0)\n",
    "gradient(z::Array{Float64, 2}, act_fn::ReLU)::Array{Float64,2} = Array{Float64, 2}(z.>0)\n",
    "\n",
    "struct Softmax\n",
    "end;\n",
    "forward(z::Array{Float64, 2}, act_fn::Softmax)::Array{Float64,2} = softmax(z)\n",
    "gradient(z::Array{Float64, 2}, act_fn::Softmax)::Array{Float64,2} = (softmax(z)) .* (1 .- softmax(z))\n",
    "\n",
    "function softmax(z::Array{Float64,2})::Array{Float64,2}\n",
    "    #converts real numbers to probabilities\n",
    "    c=maximum(z)\n",
    "    p = z .- log.( sum( exp.(z .- c) ) ) .-c\n",
    "    p = exp.(z)\n",
    "    return p\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e7db825-3e5e-486a-9f24-ab71932a1d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"activations.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887dc5a0-687c-4656-bbda-2c6551c179c1",
   "metadata": {},
   "source": [
    "The ReLU function’s forward method simply returns the input value if it’s more than zero, else zero. The ${\\color{Yellow}gradient}$ is one if the input is more than zero, else zero.\n",
    "\n",
    "$$\\text{softmax}(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^m e^{z_j}}$$\n",
    "$$\\text{The softmax function, element-wise}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657296d3-28a5-4ce2-97da-3acbfac8f4f1",
   "metadata": {},
   "source": [
    "The softmax function acting on a vector, _softmax(z)_, returns a normalised pointwise exponential of the vector’s elements, such that the output elements sum to one. Its derivative with respect to its input _z_, takes the form _grad_softmax(z) = softmax(z) * (1 — softmax(z))_ , where 1 is the appropriate vector of ones, and * is the Hadamard product ( _.*_ in Julia). In this implementation, we utilise the so-called ‘log-sum-exp’ trick for numerical stability when calculating the softmax. There’s a great explanation of this technique [here](https://blog.feedly.com/tricks-of-the-trade-logsumexp/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb9e483-8ad7-46cb-86fc-e526a22f4e04",
   "metadata": {},
   "source": [
    "The softmax function ensures that an output vector’s elements are all > 0, real valued, and sum to one. These properties are synonymous with a valid probability distribution over the vector’s elements, which we choose to use as a model for the probabilty of an input vector **x**’s assignment to one of the classes represented by the output vector **y**’s elements. Under this model, the inputs to the softmax function are the log-odds associated with each class. This is essentially all we need to build the functional form of our MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3188b2-6cd6-4c8c-bb10-0e9a0a03a8ba",
   "metadata": {},
   "source": [
    "#### Putting it together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fadf13-e5f0-4417-8e42-26e32a9210f7",
   "metadata": {},
   "source": [
    "When training the neural network we’ll need to keep track of intermediate activations as well as the values of the parameters themselves, and here we simply utilise a _dict_ (to allow O(1) access to any element) to do so:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bc755c-5d6c-426b-ae18-c7ff932b1994",
   "metadata": {},
   "source": [
    "```julia\n",
    "# The net is simply a dictionary containing parameters,\n",
    "# activations post- (A) and pre- (Z) activation function.\n",
    "net=Dict(\"Layers\"=>[], \"A\"=>[], \"Z\"=>[])\n",
    "\n",
    "dims=[[28^2, 32] [32, 32] [32, 10]]\n",
    "layers=[]\n",
    "for i in 1:size(dims,2)-1\n",
    "    append!(layers, [Dense(dims[1,i], dims[2,i], ReLU())])\n",
    "end\n",
    "\n",
    "head=[Dense(dims[1, size(dims,2)], dims[2, size(dims,2)], Softmax())]\n",
    "append!(layers, head);\n",
    "net[\"Layers\"]=layers;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "730a7029-b319-452f-bbbf-836309efe304",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"neural_net.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d8b22c-b31d-4b0f-b8a9-09ad987a03ed",
   "metadata": {},
   "source": [
    "So here we define a net dictionary, with fields “Layers”, “A”, and “Z”. “Z” and “A” are required for the backpropagation algorithm, as you’ll see later.\n",
    "\n",
    "- “Layers” is a list of the individual Dense layers.\n",
    "- “A” tracks the values of the layers’ activations through a forward pass after applying the activation function.\n",
    "- “Z” tracks the activations prior to applying the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff28b08-0030-4f33-98c7-caca75c1fbaa",
   "metadata": {},
   "source": [
    "Structurally, the final thing we need to do is define a _forward()_ method for our MLP, again utilising native **Julia** dynamic dispatch. We simply loop through the stored layers in our _dict_, multiplying the input by the weight matrix, adding the bias, and applying our pointwise activation function. We keep track of the activations both before and after applying the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36acb7ff-25f6-4f2b-b678-7ecccb28f90a",
   "metadata": {},
   "source": [
    "```julia\n",
    "function forward(x::Array{Float64,2}, net::Dict)::Array{Float64,2}\n",
    "    # Feed the input through a network.\n",
    "    # Keep track of the activations both pre- and post- activation function.\n",
    "    # Store these values in the network's dictionary.\n",
    "    A=[x]\n",
    "    Z=[]\n",
    "    for n in 1:length(net[\"Layers\"])\n",
    "        z = net[\"Layers\"][n].weight*x + net[\"Layers\"][n].bias\n",
    "        x = forward(z, net[\"Layers\"][n].act_fn)\n",
    "        append!(Z, [z])\n",
    "        append!(A, [x])\n",
    "    end\n",
    "    net[\"A\"]=A\n",
    "    net[\"Z\"]=Z\n",
    "    return x\n",
    "end; \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da917085-3380-4cfc-bd0c-dce2bb9f5405",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"forward.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bcd4bf-498e-4f89-9598-50cba67faf9f",
   "metadata": {},
   "source": [
    "So we’re in a good place. We can call _y = forward(x, net)_ to generate a set of predictions for the class of _x_, though this is obviously random at the moment as we have yet to train the network. Time for backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5ba072-fca8-447f-8b16-a70f93480a9a",
   "metadata": {},
   "source": [
    "## Part 2 — Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc90c84-72fc-4209-84bf-944e7f9b48fd",
   "metadata": {},
   "source": [
    "We have built the functional form of our MLP: _y = forward(x, net)_ ouputs a 10 x 1 vector _y_ from a 784 x 1 vector _x_. We’ll go into the backpropagation algorithm for updating the net’s parameters shortly, but first we need to talk data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e1407d-94aa-424b-a398-d23b0d445a35",
   "metadata": {},
   "source": [
    "#### Loading and loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516d96ab-1e38-4285-903c-d2830f4c1611",
   "metadata": {},
   "source": [
    "After installing, we can import with _using MLDatasets_. We can then _calltrain_x, train_y = MNIST.traindata(Float64)_. The _x_ data is good to go from here, but we need to use “one-hot encoding” for the _y_ data, as _train_y_ by default outputs an integer between 0 and 9 as the class label. We do this using the function _zi_one_hot_encode(train_y)_ :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235ff715-ccc4-4aca-ab0b-e4002b26a762",
   "metadata": {},
   "source": [
    "```julia\n",
    "function zi_one_hot_encode(data)\n",
    "  \n",
    "    # assumes zero-indexed categories\n",
    "    one_hot=zeros(Float64, maximum(data)+1, size(data, 1));\n",
    "  \n",
    "    for i in 1:size(data, 1)\n",
    "        label=data[i]+1\n",
    "        one_hot[label, i]=1\n",
    "    end;\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "end;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53b72063-fdc9-472d-a11c-af5ceb65f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"zi_one_hot.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eb379c-2877-4635-8dff-0fe78bd67ca0",
   "metadata": {},
   "source": [
    "As discussed previously, our _Softmax_ layer outputs a vector representing a probability distribution over classes, ie. its elements are real valued, > 0, and sum to one. We would like the output of our MLP, a vector y representing a probability distribution over classes, to be as close possible to the one-hot encoding of the training data. We can then minimise the cross-entropy between these probability distributions to train our neural network.\n",
    "\n",
    "$$\\mathcal{L} = -\\frac{1}{m} \\sum_{i=1}^m Y_i \\; log \\; y_i$$\n",
    "$$\\text{Cross-entropy loss}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9ef215-60fa-4d3c-8c45-6694417c170a",
   "metadata": {},
   "source": [
    "For a deeper dive into the cross-entropy loss, and its (close) relation to the Kullback-Leibler divergence (the _relative_ entropy), see [here](https://machinelearningmastery.com/cross-entropy-for-machine-learning/). It’s the maximum likelihood function for a Bernoulli distribution over classes (corresponding to our one-hot encoding), so is the correct loss function for single-label classification. Here’s a Julia implementation, along with its derivative which we’ll need for backpropagation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d60c8f0-0abb-4192-ba8c-5d96836ddd10",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "```julia\n",
    "# xe_losses assume inputs as probabilities (real valued and \n",
    "# normalised, ie. after a Softmax).\n",
    "xe_loss(y::Array{Float64,2}, Y::Array{Float64,2}) = - sum(Y.*log.(y))\n",
    "xe_loss_derivative(y::Array{Float64,2}, Y::Array{Float64,2}) = y - Y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee259f3d-92f6-44f6-b808-070c43a9dbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"cross_entropy.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0d52a2-3722-4f97-acfe-c4bd8299edb6",
   "metadata": {},
   "source": [
    "So we’re finally ready to tackle the big beast — backpropagation. I found [this](https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795) blog post by Piotr Skalski tremendously helpful when tackling this, and a lot of my implementation of this algorithm is inspired by his work. I’ve used the same syntax, so check out his explanation if mine doesn’t do it for you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79f52fc-6618-4fb2-ab18-b4e43d78192b",
   "metadata": {},
   "source": [
    "# References\n",
    "- [ ] [Training a ground-up MLP in native Julia (yes, on MNIST).](https://tmcauliffe.medium.com/training-a-ground-up-mlp-in-native-julia-yes-on-mnist-c2f84aaca2f5)\n",
    "- [ ] [MNIST database](https://en.wikipedia.org/wiki/MNIST_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66734d7b-72f0-48f5-944d-3fb0af1c0ff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
