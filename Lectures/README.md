
# Lectures

Lecture's content and syllabus

&#x1F4D1;  [Lecture 1: Introduction [Oct. 6]](1)

&#x1F4D1;  [Lecture 2: Linear Models for Regression [Oct 13]](2)

&#x1F4D1;  [Lecture 3: Linear Classifiers: LDA & Logistic Regression [Oct 18]](3)

&#x1F4D1;  [Lecture 4: Gradient Descent [Oct 27]](4)

&#x1F4D1;  [Lecture 5: The Perceptron & Bias-Variance Decomposition [Nov 10]](5)

&#x1F4D1;  [Lecture 6: Support Vector Machines & Kernels [Nov 17]](6)

&#x1F4D1;  [Lecture 7: Kernels [Nov 24]](7)

&#x1F4D1;  [Lecture 8: Kernels (cont), Bias-variance decomposition and regularization [Dec 1]](8)

&#x1F4D1;  [Lecture 9: Regularization, Validation & Trees (Part I) [Dec 8]](9)

&#x1F4D1;  [Lecture 10: Ensembles [Jan 12]](_10)

&#x1F4D1;  [Lecture 11: Unsupervised Learning [Jan 26]](_11)

&#x1F4D1;  [Lecture 12: Neural Networks - Course Review [Jan 26]](_12)

# References

```math
\begin{gather}
\phi(u) = [u_i^2, \sqrt(2Cu_1), c]^T
\\
\phi(v) = [v_i^2, \sqrt(2Cv_1), c]^T
\\
\phi(u)^T\phi(v) = u_i^2 v_i^2 + 2cu_iv_i + c^2
\end{gather}
```
