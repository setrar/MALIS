{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7740b85e-4b89-45dd-af1c-9821785468f9",
   "metadata": {},
   "source": [
    "# Definition\n",
    "\n",
    "\n",
    "$$ \\text{ We have }{\\color{Orange}y} \\in \\mathcal{C} \\text{ and } {\\color{Orange}x} \\in \\mathbb{R}^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492d764e-64a7-4c27-9c12-a9434fa0055b",
   "metadata": {},
   "source": [
    "- Step 0) Find a distribution\n",
    "- Step 1) plug the PDF\n",
    "- Step 2) Take the log\n",
    "- Step 3) Derive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bba4ac1-1cc1-460a-bbe5-5fc4d49b9017",
   "metadata": {},
   "source": [
    "### Solving the OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec09722d-b9a6-40ef-ae47-fd56312075c7",
   "metadata": {},
   "source": [
    "To find the derivative $\\frac{\\partial}{\\partial W}$ of the expression $\\frac{1}{N} \\cdot (y - XW)^T(y - XW)$ with respect to the vector $W$, where $y$ is a vector, $X$ is a matrix, and $W$ is a vector, you can use the chain rule and matrix calculus. The result will be a gradient vector representing the partial derivatives of the expression with respect to each element of $W$.\n",
    "\n",
    "Let's compute it step by step:\n",
    "\n",
    "Given: \n",
    "- $y$ is a column vector of size $N \\times 1$.\n",
    "- $X$ is a matrix of size $N \\times M$.\n",
    "- $W$ is a column vector of size $M \\times 1$.\n",
    "\n",
    "The expression is:\n",
    "\n",
    "$L(W) = \\frac{1}{N} \\cdot (y - XW)^T(y - XW)$\n",
    "\n",
    "Now, let's find the derivative $\\frac{\\partial L}{\\partial W}$ using matrix calculus:\n",
    "\n",
    "Step 1: Expand the expression:\n",
    "\n",
    "$L(W) = \\frac{1}{N} \\cdot (y^Ty - y^TXW - W^TX^Ty + W^TX^TXW)$\n",
    "\n",
    "$L(W) = \\frac{1}{N} \\cdot (y^Ty - 2y^TXW + W^TX^TXW)$\n",
    "\n",
    "Step 2: Compute the derivative:\n",
    "\n",
    "To find $\\frac{\\partial L}{\\partial W}$, we'll compute the derivatives of each term separately and then apply the chain rule.\n",
    "\n",
    "a) Derivative of the first term (constant with respect to $W$) is zero.\n",
    "\n",
    "b) Derivative of the second term:\n",
    "\n",
    "$\n",
    "\\frac{\\partial}{\\partial W} (-2y^TXW) = -2X^Ty\n",
    "$\n",
    "\n",
    "c) Derivative of the third term:\n",
    "\n",
    "$\n",
    "\\frac{\\partial}{\\partial W} (W^TX^TXW) = 2X^TXW\n",
    "$\n",
    "\n",
    "Now, summing up the derivatives:\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial W} = -2X^Ty + 2X^TXW\n",
    "$\n",
    "\n",
    "Step 3: Set the derivative equal to zero:\n",
    "\n",
    "$\n",
    "-2X^Ty + 2X^TXW = 0\n",
    "$\n",
    "\n",
    "Step 4: Solve for $W$:\n",
    "\n",
    "Now, we want to solve for $W$. First, move the first term to the other side of the equation:\n",
    "\n",
    "$\n",
    "2X^TXW = 2X^Ty\n",
    "$\n",
    "\n",
    "Then, divide both sides by $2X^TX$ (assuming that $2X^TX$ is invertible):\n",
    "\n",
    "$\n",
    "\\hat{W} = (X^TX)^{-1}X^Ty\n",
    "$\n",
    "\n",
    "This is the solution for $W$ that minimizes the expression $1/N \\cdot (y - XW)^T(y - XW)$ with respect to $W$ when the derivative is set equal to zero. It represents the ordinary least squares estimate of $W$ in linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea725cb3-d2bc-41ff-aaac-bda7b380a8fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
