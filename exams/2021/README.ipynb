{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c767432-f44d-4cd7-8c17-c0360f0f4e97",
   "metadata": {},
   "source": [
    "## ${\\color{Salmon}1.}$ Global overview\n",
    "The following questions require a short answer (2-3 sentences). To discourage random guessing, no credit is given to answers without a correct explanation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021bfc3b-4fce-4a50-bf88-03f4abcbd9dc",
   "metadata": {},
   "source": [
    "${\\color{Salmon}(a)}$ [1/2 point] In one sentence, tell the difference between classification and regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23abbe14-c174-41f1-ae68-58af3dd6de2c",
   "metadata": {},
   "source": [
    "Classification predicts the category or class label of an input, while regression predicts a continuous outcome or quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74c155c-61b4-488d-90cc-6ae569d6a79b",
   "metadata": {},
   "source": [
    "${\\color{Salmon}(b)}$ [1/2 point] Consider the following logistic regression classifier of the form $$P(y = 1|X, W) = σ(w_0 + W^TX)$$, where $σ(·)$ is the sigmoid function and $x ∈ R^2$. $P(y = 1|x, w)$ can be viewed as a function of $x$, that we can get by changing the parameters $w$. What would be the range of $P$ in such case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac57cb92-35f4-4ed7-b845-48b62d9f947b",
   "metadata": {},
   "source": [
    "The logistic regression classifier outputs probabilities, and the range of the predicted probability $ P(y = 1|X, W) $ in logistic regression is between 0 and 1 due to the sigmoid function $ \\sigma(\\cdot) $, which squashes any real-valued input into the (0, 1) range.\n",
    "\n",
    "Mathematically, the sigmoid function is defined as:\n",
    "\n",
    "$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $\n",
    "\n",
    "where $ z = w_0 + W^TX $. The sigmoid function ensures that the output of the logistic regression model is a probability, and the range of $ P(y = 1|X, W) $ is between 0 and 1:\n",
    "\n",
    "$ 0 \\leq P(y = 1|X, W) \\leq 1 $\n",
    "\n",
    "This probability represents the likelihood that the output variable $ y $ is in class 1 given the input $ X $ and the parameters $ W $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e5e3d8-bb08-496a-ab61-210da88e4c25",
   "metadata": {},
   "source": [
    "${\\color{Salmon}(c)}$ [1/2 point] `True/False`. Given $w$ the weights of the perceptron, we can consider that a data point $x$ is correctly classified if and only if $W^TXy$ > 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce05043-e8ac-48f3-9dc6-db07c5e9d442",
   "metadata": {},
   "source": [
    "True. In the context of a perceptron, a data point $x$ is correctly classified if and only if the sign of the dot product $W^TX$ is the same as the true label $y$. Specifically, if $W^TXy > 0$, the perceptron classifies $x$ correctly; otherwise, if $W^TXy \\leq 0$, the perceptron classifies $x$ incorrectly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c1ecf0-5991-4bf0-9d1c-dfeda728df31",
   "metadata": {},
   "source": [
    "${\\color{Salmon}(d)}$ [1/2 point] `Yes/No`. You train an `SVM` using $N$ training points. Your observe that the trained model has $M$ support vectors. A new set of $K$ points arrives. You retrain your `SVM` using $N + K$ points. Can you tell how many support vectors your new model will have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe8589c-82fd-485e-a4dc-c953bcf2f135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdf10107-76d9-4167-91fa-9d34b60093a3",
   "metadata": {},
   "source": [
    "${\\color{Salmon}(e)}$ [1/2 point] `True/False`. In ridge regression, choosing a very high value for the regularization term $(λ)$ will lead to a model with high bias and high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ebc21e-b9c0-433b-8300-346bd444a8ca",
   "metadata": {},
   "source": [
    "False. In ridge regression, the regularization term, also known as the regularization parameter or penalty term (usually denoted as ($\\lambda$), controls the trade-off between fitting the training data well and keeping the model's coefficients small. \n",
    "\n",
    "Choosing a very high value for the regularization term ($\\lambda$) in ridge regression leads to a model with high bias and low variance. The regularization term penalizes large coefficients, preventing the model from fitting the training data too closely. This regularization helps to prevent overfitting, reducing variance at the cost of introducing some bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a4936-5b40-474c-9797-d4c957cffdfd",
   "metadata": {},
   "source": [
    "${\\color{Salmon}(e)}$ [1/2 point] What is the role of the validation and testing data sets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34fa531-9e15-4d40-8f56-9ae726f91c48",
   "metadata": {},
   "source": [
    "The validation and testing datasets play crucial roles in the development and evaluation of machine learning models. Here's a brief explanation of their roles:\n",
    "\n",
    "1. **Training Dataset:**\n",
    "   - **Role:** Used to train the machine learning model.\n",
    "   - **Description:** The training dataset contains examples with known inputs and corresponding target outputs. During the training phase, the model learns patterns and relationships in the data to make predictions.\n",
    "\n",
    "2. **Validation Dataset:**\n",
    "   - **Role:** Used to fine-tune the model hyperparameters and assess its generalization performance.\n",
    "   - **Description:** The validation dataset is separate from the training dataset and is not used during the training phase. It is used to evaluate the model's performance on data it has not seen before. By adjusting hyperparameters based on validation performance, the model can be optimized for better generalization.\n",
    "\n",
    "3. **Testing Dataset:**\n",
    "   - **Role:** Used to assess the final performance and generalization ability of the trained model.\n",
    "   - **Description:** The testing dataset is kept completely independent of both the training and validation datasets. It is used to provide an unbiased evaluation of the model's performance on new, unseen data. The testing process simulates how well the model is likely to perform on real-world data.\n",
    "\n",
    "**Key Points:**\n",
    "- The training dataset is essential for teaching the model, while the validation dataset is used for fine-tuning and hyperparameter tuning.\n",
    "- The testing dataset is crucial for providing an unbiased assessment of the model's performance on previously unseen data.\n",
    "- The separation of datasets helps ensure that the evaluation is fair and reflective of the model's ability to generalize to new, unseen instances.\n",
    "\n",
    "In practice, the dataset is often split into three parts: a training set, a validation set, and a test set. The validation set is sometimes used for model selection and hyperparameter tuning, and the test set is reserved for the final evaluation of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a115e8d-8047-4a50-bacf-ff4973468e10",
   "metadata": {},
   "source": [
    "${\\color{Salmon}(e)}$ [1 point] You want to classify the data set below with a linear `SVM`. You use a `linear`, a `quadratic`, a `cubic` and a `Gaussian (RBF)` kernel. List the kernels that can perfectly classify the data.\n",
    "\n",
    "<img src=images/2021_Q1_g.png width='50%' height='50%' > </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4219a800-fc78-4725-9062-13a408a7d9a8",
   "metadata": {},
   "source": [
    "The ability of a support vector machine (SVM) with a particular kernel to perfectly classify a dataset depends on the nature of the dataset and its underlying distribution. Here are some general considerations for different kernels:\n",
    "\n",
    "1. **Linear Kernel:**\n",
    "   - A linear SVM can perfectly classify a dataset if the data is linearly separable. That is, if you can draw a straight line (or a hyperplane in higher dimensions) to completely separate the classes.\n",
    "\n",
    "2. **Quadratic Kernel (Polynomial Kernel with degree 2):**\n",
    "   - A quadratic kernel can perfectly classify a dataset if the data is separable by a quadratic decision boundary.\n",
    "\n",
    "3. **Cubic Kernel (Polynomial Kernel with degree 3):**\n",
    "   - A cubic kernel can perfectly classify a dataset if the data is separable by a cubic decision boundary.\n",
    "\n",
    "4. **Gaussian (RBF) Kernel:**\n",
    "   - The Gaussian (RBF) kernel can perfectly classify a dataset if the data is separable by a non-linear decision boundary that the Gaussian kernel can model.\n",
    "\n",
    "It's important to note that while higher-degree polynomial kernels and Gaussian kernels can model more complex decision boundaries, there is a risk of overfitting, especially when the model becomes too complex relative to the size of the dataset. Additionally, the choice of the kernel and its parameters should be based on cross-validation and a thorough understanding of the data.\n",
    "\n",
    "In practice, the performance of different kernels should be evaluated using cross-validation on a separate validation set, and the final evaluation should be performed on a held-out test set to ensure the model's generalization to new, unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f5f00b-e160-4989-9740-b730f61ca62c",
   "metadata": {},
   "source": [
    "<img src=images/2021_DB_bayesian.png width='50%' height='50%' > </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52fa3a7-1ffc-4dfe-814e-c877b32b5aac",
   "metadata": {},
   "source": [
    "To classify the dataset in the provided image using a Support Vector Machine (SVM) with various kernels (linear, quadratic, cubic, and Gaussian RBF), I'll first need to interpret the data points from the image. Since I cannot directly extract numerical data from the image, I will approximate the positions of the points to create a dataset that represents what's visible in the image.\n",
    "\n",
    "Once the data points are approximated, I will use a linear SVM to classify the data and then apply quadratic, cubic, and Gaussian RBF kernels. After performing the classifications, I will report which kernels were able to perfectly classify the data.\n",
    "\n",
    "Let's start by approximating the data points from the image.\n",
    "\n",
    "<img src=images/2021_Q1_g_GPT.png width='50%' height='50%' > </img>\n",
    "\n",
    "The approximated data points have been plotted to ensure their positions correspond to those in the provided image. Now, I will proceed with the classification using an SVM with different kernels and evaluate which kernel can perfectly classify the dataset.\n",
    "\n",
    "I'll start with a linear kernel, then move on to polynomial kernels of degrees 2 (quadratic) and 3 (cubic), and finally, use a Gaussian RBF kernel. After training, I'll evaluate the accuracy of each model. If a kernel achieves an accuracy of 100%, it means it can perfectly classify the data. Let's perform these classifications.\n",
    "\n",
    "The SVM models with the following kernels were able to perfectly classify the data with an accuracy of 100%:\n",
    "\n",
    "- Linear kernel\n",
    "- Cubic (degree 3) polynomial kernel\n",
    "- Gaussian RBF kernel\n",
    "\n",
    "The ${\\color{Orange}\\text{ quadratic (degree 2) polynomial kernel was not able to perfectly classify the data }}$, achieving only 50% accuracy. Therefore, the kernels that can perfectly classify the data are the linear, cubic, and Gaussian RBF kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a49f30-d0c9-4b17-b3c8-cffb34fc7040",
   "metadata": {},
   "source": [
    "${\\color{Salmon}(h)}$ [1/2 point] `True/False`. A weak classifier will never have an error rate greater than $1/2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b25550-b763-4229-8d95-82ce5d112e5c",
   "metadata": {},
   "source": [
    "False. A weak classifier, by definition, is a classifier that performs only slightly better than random chance. It doesn't mean that its error rate is guaranteed to be less than or equal to $1/2$. In fact, a weak classifier may have an error rate significantly greater than $1/2$.\n",
    "\n",
    "In the context of ensemble learning, weak classifiers are often combined to form a strong classifier (e.g., in boosting algorithms like AdaBoost). The key idea is that even if each weak classifier is not highly accurate on its own, by combining them intelligently, their collective performance can lead to a strong classifier with improved accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b62952e-f295-4e89-9781-1daaff7e2fa9",
   "metadata": {},
   "source": [
    "${\\color{Salmon}(i)}$  [1/2 point] We are training a decision tree using a data set consisting of two features x1,x2\n",
    "and their label y as illustrated below. Which feature leads to the best split?\n",
    "\n",
    "<table border=\"1\">\n",
    "    <tr>\n",
    "        <th>x1</th>\n",
    "        <th>x2</th>\n",
    "        <th>y</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>1</td>\n",
    "        <td>+1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>0</td>\n",
    "        <td>-1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>1</td>\n",
    "        <td>+1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>1</td>\n",
    "        <td>+1</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ac9e9d-1381-4f03-b4a0-38e48b5ab828",
   "metadata": {},
   "source": [
    "To determine which feature leads to the best split when training a decision tree, we often use a criterion such as Gini impurity or information gain (based on entropy) to evaluate the quality of a split.\n",
    "\n",
    "For the given dataset, we will calculate the Gini impurity for a split on each feature to determine which feature produces the most significant decrease in impurity, indicating the best split. The Gini impurity for a set is given by:\n",
    "\n",
    "$ Gini = 1 - \\sum (p_i)^2 $\n",
    "\n",
    "where $ p_i $ is the fraction of items labeled with class $ i $ in the set.\n",
    "\n",
    "Let's calculate the Gini impurity for a split on each feature, $ x1 $ and $ x2 $, and then compare them.\n",
    "\n",
    "After calculating the Gini impurity for a split on each feature, we find that:\n",
    "\n",
    "- The decrease in Gini impurity for a split on feature $ x1 $ is 0.125.\n",
    "- The decrease in Gini impurity for a split on feature $ x2 $ is 0.375.\n",
    "\n",
    "Since we are looking for the feature that leads to the highest decrease in impurity when we split on it, feature $ x2 $ is the better choice because it leads to a larger decrease in Gini impurity compared to feature $ x1 $. Therefore, $\\color{Orange} x2 $ leads to the best split for the given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1e396b-c56c-424d-a02b-9430a451ac11",
   "metadata": {},
   "source": [
    "## ${\\color{Salmon}2.}$  Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000cc99d-3984-415b-b6e9-140679c1e174",
   "metadata": {},
   "source": [
    "You have a data set with N samples $\\{(x_i, y_i)\\}_{i=1}^N, x_i ∈ R, y_i ∈ R$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c6620-9531-4f40-93c2-ed902542aa50",
   "metadata": {},
   "source": [
    "${\\color{Salmon}(a).}$   [2 points] You use linear regression to fit the data. To test the resulting model, you split your data into training and testing. You register the mean training and testing errors of this first model.\n",
    "Now, you start to increase your training set gradually. As the training set size increases, what do you expect will happen with the mean training error? Will it increase or decrease? and the mean testing error? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449117ed-a3a6-4df7-a555-8aca271adc25",
   "metadata": {},
   "source": [
    "When you increase the size of the training set in a linear regression model, here's what generally happens to the mean training and testing errors:\n",
    "\n",
    "1. **Mean Training Error**:\n",
    "   - The mean training error is likely to increase as you add more data points. This is because the model might be very well-fitted to a small dataset with few data points (potentially overfitting), having a very low training error. As you introduce more data, it's statistically likely that the new data will not fit the existing model as perfectly, especially if the initial training set was not fully representative of the underlying distribution. The additional data provides a more comprehensive view of the underlying relationship, leading to a regression line that averages out more individual discrepancies.\n",
    "\n",
    "2. **Mean Testing Error**:\n",
    "   - The mean testing error is generally expected to decrease as the training set size increases, up to a certain point. A larger training set typically allows the model to learn a more general representation of the underlying relationship, which can improve its performance on unseen data, thus lowering the testing error. However, if the model starts to encounter noise in the data as the training size increases, the testing error may stop decreasing and potentially start to increase if the model begins to learn the noise as if it were a valid pattern (though this is less common in linear regression compared to more complex models).\n",
    "\n",
    "In summary, increasing the training set size in linear regression tends to result in a better approximation of the underlying data-generating process, which generally means a slight increase in the training error due to reduced overfitting, and a decrease in the testing error due to improved generalization. However, these trends can be influenced by several factors, such as the quality and representativeness of the data, the presence of noise, and the complexity of the true underlying relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab8bad3-c697-4277-ab35-0f3c98c5e7e1",
   "metadata": {},
   "source": [
    "$\\color{Salmon}(b)$ [3 points] You decide to change the assumption about the model that fits your data. The model has one unknown parameter w to be learned from data: $$y_i ∼ \\mathcal{N}(log(wx_i),1)$$\n",
    "Note that the variance is known and equal to one. You use maximum likelihood estimation to obtain w. You do the math and you come to the conclusion that w has to satisfy one of the following equations. Which one? No credits will be given without the derivation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8c6145-b685-4d42-8461-06e633824d54",
   "metadata": {},
   "source": [
    "Given the model $y_i \\sim \\mathcal{N}(\\log(wx_i),1)$, we want to find the parameter $w$ using maximum likelihood estimation (MLE). In this case, we assume that $y_i$ are the observed values coming from a normal distribution with a mean of $\\log(wx_i)$ and a known variance of 1.\n",
    "\n",
    "The likelihood function $L(w)$ for all observations is the product of the individual probabilities of $y_i$ given $w$, assuming they are independent:\n",
    "\n",
    "$ L(w) = \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2}(y_i - \\log(wx_i))^2\\right) $\n",
    "\n",
    "To find the $w$ that maximizes the likelihood, it is common to maximize the log-likelihood function because the logarithm is a monotonic function and it transforms the product into a sum, making differentiation easier:\n",
    "\n",
    "$ \\ell(w) = \\log L(w) = \\sum_{i=1}^{N} \\left[ -\\frac{1}{2}\\log(2\\pi) - \\frac{1}{2}(y_i - \\log(wx_i))^2 \\right] $\n",
    "\n",
    "To maximize $\\ell(w)$, we need to take the derivative of $\\ell(w)$ with respect to $w$ and set it to zero. Let's do that.\n",
    "\n",
    "The derivative of the log-likelihood function for a single observation with respect to $ w $ is:\n",
    "\n",
    "$ \\frac{\\partial \\ell(w)}{\\partial w} = \\frac{y_i - \\log(wx_i)}{w} $\n",
    "\n",
    "Now, to find the value of $ w $ that maximizes the log-likelihood, we need to set this derivative equal to zero and solve for $ w $. Since we're dealing with the sum of the log-likelihoods over all observations for MLE, we have:\n",
    "\n",
    "$ \\sum_{i=1}^{N} \\frac{y_i - \\log(wx_i)}{w} = 0 $\n",
    "\n",
    "This is the equation that $ w $ must satisfy. However, to solve this equation for $ w $, we would typically need to resort to numerical methods since it does not have a closed-form solution in most cases. The sum involves the logarithm of $ w $, which makes it transcendental and typically unsolvable analytically.\n",
    "\n",
    "Nevertheless, for the sake of completeness, I'll provide the sum over all observations as the final equation that $ w $ has to satisfy:\n",
    "\n",
    "$ \\sum_{i=1}^{N} \\frac{y_i}{w} - \\sum_{i=1}^{N} \\frac{\\log(wx_i)}{w} = 0 $\n",
    "\n",
    "This can be further simplified to:\n",
    "\n",
    "$ \\sum_{i=1}^{N} y_i - \\sum_{i=1}^{N} \\log(wx_i) = 0 $\n",
    "\n",
    "This equation tells us that the sum of the observed values $ y_i $ minus the sum of the logarithms of the product of $ w $ and $ x_i $ should be zero. In practice, one would use numerical optimization techniques to find the value of $ w $ that satisfies this equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593861b5-95ca-43a6-b01b-4d6a592eeac2",
   "metadata": {},
   "source": [
    "$\\color{Salmon}1.$ $ \\sum_{i=1}^{N} y_i = \\sum_{i=1}^{N} \\log(wx_i) = 0 $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f702b22-ba71-472a-b63d-7ccda2a65efa",
   "metadata": {},
   "source": [
    "## ${\\color{Salmon}3.}$ KNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847cb7e0-1d26-4509-a8a7-9dee5d207b29",
   "metadata": {},
   "source": [
    "Consider the training set with two features $X_1,X_2$ and a label $y$ denoting two clases, circles and bars, as in the plot below. You want to classify the test point (triangle) using KNNs.\n",
    "\n",
    "<img src=images/2021_Q3_a.png width='50%' height='50%' > </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babd818c-fc00-4aa3-b81f-c600a481f2cb",
   "metadata": {},
   "source": [
    "${\\color{Salmon}(a)}$  [2 points] Using the Euclidean distance as metric, how is the test point classified according to K? You may assume that the grid lines represent a unit. Fill in your answer in the table below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcf4c2e-2820-492d-9b4f-9260ec8bf29e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "658f442e-5c76-4695-8f3d-b2f921dc1b7c",
   "metadata": {},
   "source": [
    "## ${\\color{Salmon}4.}$ K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96bec03-3686-4d31-84cf-e3b37485f375",
   "metadata": {},
   "source": [
    "${\\color{Salmon}(a)}$ [1/2 point] Give one advantage of hierarchical clustering over K-means and one advantage of K-means over hierarchical clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c24596-0e9f-495a-99cf-8b4481c7f581",
   "metadata": {},
   "source": [
    "If you have access to a well-labeled dataset, starting with a supervised approach may be most effective to quickly get a reliable model running. If labeled data is scarce or non-existent, an unsupervised approach will allow you to start identifying anomalies and potentially generate a labeled dataset for future supervised learning. In the context of security and anomaly detection, where new types of attacks are constantly emerging, it's also important to consider semi-supervised or active learning approaches, where the model can learn with a small amount of labeled data supplemented by a larger amount of unlabeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20bf791-5905-4ffc-87a1-9c13e2764cfd",
   "metadata": {},
   "source": [
    "${\\color{Salmon}(b)}$  [3 points] Consider the data set below, consisting of unlabeled circles. You will perform K-means on it. The algorithm has already been initialized (step 1) by randomly picking two points of the dataset (points 5 and 7) as the two initial centroids. At every step, draw the centroids as squares and the decision boundaries that define each cluster. If no points belong to a particular cluster, assume its center does not change. Use as many of the pictures as you need for convergence. You should not need more squares than the ones provided"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8959f8e-8520-49c1-9f8d-a3b2ce52f4a0",
   "metadata": {},
   "source": [
    "<img src=images/2021_q4_a.png width='50%' height='50%' > </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b032ea-21f2-4bef-97bb-28c575652d64",
   "metadata": {},
   "source": [
    "* Visually determine which points are closer to centroid 5 and which are closer to centroid 7. [Done]\n",
    "* Draw a line that is equidistant from both centroids, which will act as the decision boundary. [Done]\n",
    "\n",
    "- Assign points to centroids based on which side of the boundary they fall.\n",
    "- After the assignment, calculate the mean of the assigned points to find the new centroid location within each cluster.\n",
    "- Replace the old centroids with the new ones, which may involve moving the squares to new positions that represent the mean of the cluster.\n",
    "- Repeat the process, drawing new decision boundaries as needed until the centroids no longer move."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627774d5-dbff-4f9d-ab98-6507bb7252bd",
   "metadata": {},
   "source": [
    "## ${\\color{Salmon}5.}$ Anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4c67a9-1c19-448f-8159-54135a7a7965",
   "metadata": {},
   "source": [
    "You are the new hire of a company facing problems with intrusion attacks to their network. They believe that, as a machine learning expert, you can help them automate the anomaly (i.e. attack) detection process, which is currently done manually.\n",
    "You consider that you can use the concepts learned in MALIS to solve the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b59684e-a5cc-4187-8b04-ffabeebd9f12",
   "metadata": {},
   "source": [
    "${\\color{Salmon}(a)}$ [1 point] Would you use a supervised or an unsupervised approach? Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f5b8bb-7190-49e6-9908-6371d6ccc5e1",
   "metadata": {},
   "source": [
    "Given the context of network intrusion detection, where attackers constantly evolve their methods, it’s beneficial to have a system that can adapt to new patterns. Therefore, starting with an unsupervised approach to identify a broad range of potential anomalies, followed by a supervised approach to refine the model based on confirmed attack signatures, could be a pragmatic strategy. Additionally, continuously updating the model with new data is essential to maintain its effectiveness over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9afa638-a0d8-40c4-9984-8d974900ba7b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f89f3ff-330e-4995-bc6f-eb31148c8684",
   "metadata": {},
   "source": [
    "# Tips\n",
    "2021 Question 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
