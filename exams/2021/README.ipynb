{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c767432-f44d-4cd7-8c17-c0360f0f4e97",
   "metadata": {},
   "source": [
    "## ${\\color{Salmon}1.}$ Global overview\n",
    "The following questions require a short answer (2-3 sentences). To discourage random guessing, no credit is given to answers without a correct explanation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021bfc3b-4fce-4a50-bf88-03f4abcbd9dc",
   "metadata": {},
   "source": [
    "${\\color{Salmon}(a)}$ [1/2 point] In one sentence, tell the difference between classification and regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23abbe14-c174-41f1-ae68-58af3dd6de2c",
   "metadata": {},
   "source": [
    "Classification predicts the category or class label of an input, while regression predicts a continuous outcome or quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74c155c-61b4-488d-90cc-6ae569d6a79b",
   "metadata": {},
   "source": [
    "${\\color{Salmon}(b)}$ [1/2 point] Consider the following logistic regression classifier of the form $$P(y = 1|X, W) = σ(w_0 + W^TX)$$, where $σ(·)$ is the sigmoid function and $x ∈ R^2$. $P(y = 1|x, w)$ can be viewed as a function of $x$, that we can get by changing the parameters $w$. What would be the range of $P$ in such case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac57cb92-35f4-4ed7-b845-48b62d9f947b",
   "metadata": {},
   "source": [
    "The logistic regression classifier outputs probabilities, and the range of the predicted probability $ P(y = 1|X, W) $ in logistic regression is between 0 and 1 due to the sigmoid function $ \\sigma(\\cdot) $, which squashes any real-valued input into the (0, 1) range.\n",
    "\n",
    "Mathematically, the sigmoid function is defined as:\n",
    "\n",
    "$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $\n",
    "\n",
    "where $ z = w_0 + W^TX $. The sigmoid function ensures that the output of the logistic regression model is a probability, and the range of $ P(y = 1|X, W) $ is between 0 and 1:\n",
    "\n",
    "$ 0 \\leq P(y = 1|X, W) \\leq 1 $\n",
    "\n",
    "This probability represents the likelihood that the output variable $ y $ is in class 1 given the input $ X $ and the parameters $ W $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e5e3d8-bb08-496a-ab61-210da88e4c25",
   "metadata": {},
   "source": [
    "${\\color{Salmon}(c)}$ [1/2 point] `True/False`. Given $w$ the weights of the perceptron, we can consider that a data point $x$ is correctly classified if and only if $W^TXy$ > 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce05043-e8ac-48f3-9dc6-db07c5e9d442",
   "metadata": {},
   "source": [
    "True. In the context of a perceptron, a data point $x$ is correctly classified if and only if the sign of the dot product $W^TX$ is the same as the true label $y$. Specifically, if $W^TXy > 0$, the perceptron classifies $x$ correctly; otherwise, if $W^TXy \\leq 0$, the perceptron classifies $x$ incorrectly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c1ecf0-5991-4bf0-9d1c-dfeda728df31",
   "metadata": {},
   "source": [
    "${\\color{Salmon}(d)}$ [1/2 point] `Yes/No`. You train an `SVM` using $N$ training points. Your observe that the trained model has $M$ support vectors. A new set of $K$ points arrives. You retrain your `SVM` using $N + K$ points. Can you tell how many support vectors your new model will have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe8589c-82fd-485e-a4dc-c953bcf2f135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdf10107-76d9-4167-91fa-9d34b60093a3",
   "metadata": {},
   "source": [
    "${\\color{Salmon}(e)}$ [1/2 point] `True/False`. In ridge regression, choosing a very high value for the regularization term $(λ)$ will lead to a model with high bias and high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ebc21e-b9c0-433b-8300-346bd444a8ca",
   "metadata": {},
   "source": [
    "False. In ridge regression, the regularization term, also known as the regularization parameter or penalty term (usually denoted as ($\\lambda$), controls the trade-off between fitting the training data well and keeping the model's coefficients small. \n",
    "\n",
    "Choosing a very high value for the regularization term ($\\lambda$) in ridge regression leads to a model with high bias and low variance. The regularization term penalizes large coefficients, preventing the model from fitting the training data too closely. This regularization helps to prevent overfitting, reducing variance at the cost of introducing some bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a4936-5b40-474c-9797-d4c957cffdfd",
   "metadata": {},
   "source": [
    "${\\color{Salmon}(e)}$ [1/2 point] What is the role of the validation and testing data sets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34fa531-9e15-4d40-8f56-9ae726f91c48",
   "metadata": {},
   "source": [
    "The validation and testing datasets play crucial roles in the development and evaluation of machine learning models. Here's a brief explanation of their roles:\n",
    "\n",
    "1. **Training Dataset:**\n",
    "   - **Role:** Used to train the machine learning model.\n",
    "   - **Description:** The training dataset contains examples with known inputs and corresponding target outputs. During the training phase, the model learns patterns and relationships in the data to make predictions.\n",
    "\n",
    "2. **Validation Dataset:**\n",
    "   - **Role:** Used to fine-tune the model hyperparameters and assess its generalization performance.\n",
    "   - **Description:** The validation dataset is separate from the training dataset and is not used during the training phase. It is used to evaluate the model's performance on data it has not seen before. By adjusting hyperparameters based on validation performance, the model can be optimized for better generalization.\n",
    "\n",
    "3. **Testing Dataset:**\n",
    "   - **Role:** Used to assess the final performance and generalization ability of the trained model.\n",
    "   - **Description:** The testing dataset is kept completely independent of both the training and validation datasets. It is used to provide an unbiased evaluation of the model's performance on new, unseen data. The testing process simulates how well the model is likely to perform on real-world data.\n",
    "\n",
    "**Key Points:**\n",
    "- The training dataset is essential for teaching the model, while the validation dataset is used for fine-tuning and hyperparameter tuning.\n",
    "- The testing dataset is crucial for providing an unbiased assessment of the model's performance on previously unseen data.\n",
    "- The separation of datasets helps ensure that the evaluation is fair and reflective of the model's ability to generalize to new, unseen instances.\n",
    "\n",
    "In practice, the dataset is often split into three parts: a training set, a validation set, and a test set. The validation set is sometimes used for model selection and hyperparameter tuning, and the test set is reserved for the final evaluation of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a115e8d-8047-4a50-bacf-ff4973468e10",
   "metadata": {},
   "source": [
    "${\\color{Salmon}(e)}$ [1 point] You want to classify the data set below with a linear `SVM`. You use a `linear`, a `quadratic`, a `cubic` and a `Gaussian (RBF)` kernel. List the kernels that can perfectly classify the data.\n",
    "\n",
    "<img src=images/2021_Q1_g.png width='50%' height='50%' > </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4219a800-fc78-4725-9062-13a408a7d9a8",
   "metadata": {},
   "source": [
    "The ability of a support vector machine (SVM) with a particular kernel to perfectly classify a dataset depends on the nature of the dataset and its underlying distribution. Here are some general considerations for different kernels:\n",
    "\n",
    "1. **Linear Kernel:**\n",
    "   - A linear SVM can perfectly classify a dataset if the data is linearly separable. That is, if you can draw a straight line (or a hyperplane in higher dimensions) to completely separate the classes.\n",
    "\n",
    "2. **Quadratic Kernel (Polynomial Kernel with degree 2):**\n",
    "   - A quadratic kernel can perfectly classify a dataset if the data is separable by a quadratic decision boundary.\n",
    "\n",
    "3. **Cubic Kernel (Polynomial Kernel with degree 3):**\n",
    "   - A cubic kernel can perfectly classify a dataset if the data is separable by a cubic decision boundary.\n",
    "\n",
    "4. **Gaussian (RBF) Kernel:**\n",
    "   - The Gaussian (RBF) kernel can perfectly classify a dataset if the data is separable by a non-linear decision boundary that the Gaussian kernel can model.\n",
    "\n",
    "It's important to note that while higher-degree polynomial kernels and Gaussian kernels can model more complex decision boundaries, there is a risk of overfitting, especially when the model becomes too complex relative to the size of the dataset. Additionally, the choice of the kernel and its parameters should be based on cross-validation and a thorough understanding of the data.\n",
    "\n",
    "In practice, the performance of different kernels should be evaluated using cross-validation on a separate validation set, and the final evaluation should be performed on a held-out test set to ensure the model's generalization to new, unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f5f00b-e160-4989-9740-b730f61ca62c",
   "metadata": {},
   "source": [
    "<img src=images/2021_DB_bayesian.png width='50%' height='50%' > </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52fa3a7-1ffc-4dfe-814e-c877b32b5aac",
   "metadata": {},
   "source": [
    "To classify the dataset in the provided image using a Support Vector Machine (SVM) with various kernels (linear, quadratic, cubic, and Gaussian RBF), I'll first need to interpret the data points from the image. Since I cannot directly extract numerical data from the image, I will approximate the positions of the points to create a dataset that represents what's visible in the image.\n",
    "\n",
    "Once the data points are approximated, I will use a linear SVM to classify the data and then apply quadratic, cubic, and Gaussian RBF kernels. After performing the classifications, I will report which kernels were able to perfectly classify the data.\n",
    "\n",
    "Let's start by approximating the data points from the image.\n",
    "\n",
    "<img src=images/2021_Q1_g_GPT.png width='50%' height='50%' > </img>\n",
    "\n",
    "The approximated data points have been plotted to ensure their positions correspond to those in the provided image. Now, I will proceed with the classification using an SVM with different kernels and evaluate which kernel can perfectly classify the dataset.\n",
    "\n",
    "I'll start with a linear kernel, then move on to polynomial kernels of degrees 2 (quadratic) and 3 (cubic), and finally, use a Gaussian RBF kernel. After training, I'll evaluate the accuracy of each model. If a kernel achieves an accuracy of 100%, it means it can perfectly classify the data. Let's perform these classifications.\n",
    "\n",
    "The SVM models with the following kernels were able to perfectly classify the data with an accuracy of 100%:\n",
    "\n",
    "- Linear kernel\n",
    "- Cubic (degree 3) polynomial kernel\n",
    "- Gaussian RBF kernel\n",
    "\n",
    "The ${\\color{Orange}\\text{ quadratic (degree 2) polynomial kernel was not able to perfectly classify the data }}$, achieving only 50% accuracy. Therefore, the kernels that can perfectly classify the data are the linear, cubic, and Gaussian RBF kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a49f30-d0c9-4b17-b3c8-cffb34fc7040",
   "metadata": {},
   "source": [
    "${\\color{Salmon}(h)}$ [1/2 point] `True/False`. A weak classifier will never have an error rate greater than $1/2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b25550-b763-4229-8d95-82ce5d112e5c",
   "metadata": {},
   "source": [
    "False. A weak classifier, by definition, is a classifier that performs only slightly better than random chance. It doesn't mean that its error rate is guaranteed to be less than or equal to \\(1/2\\). In fact, a weak classifier may have an error rate significantly greater than \\(1/2\\).\n",
    "\n",
    "In the context of ensemble learning, weak classifiers are often combined to form a strong classifier (e.g., in boosting algorithms like AdaBoost). The key idea is that even if each weak classifier is not highly accurate on its own, by combining them intelligently, their collective performance can lead to a strong classifier with improved accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c26de-02a7-4f47-ac25-343052a545b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
